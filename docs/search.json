[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "jpauldoll.github.io",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hello and welcome to this extremely minimally designed space. Sorry I didn’t make it nice for you – you deserve better.\nSince this site’s raison d’être is professional, I should say a word or two about me qua wage laborer. Currently, I’m employed by Stony Brook University as a graduate student in political science. There I’m working on my dissertation under the advisement of Michael Peress. If there’s (still) no tab on this site for ‘Research’ that means that its contents are still under lock and key. At Stony Brook I’ve had the pleasure of taking courses with such luminaries as Stanley Feldman, Andy Delton, Yanna Krupnikov, and Vittorio Merola. I’ve also taught a few ‘methods’ courses (coding in R, statistics, math camp, experiments in political science). Immediately before landing on the Ye Longe Isle of Bagels & Hockey Fanatics, I taught English for a few years in Lima, Peru. And before that, I triple majored in philosophy, psychology, and German at the University of Arkansas (with a year spent at Karl-Franzens Universität in Graz, Austria)."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "At Stony Brook I’ve been the ‘instructor of record’ for Introduction to Statistics (undergraduate and graduate) and Experiments in Political Science (undergraduate). In addition to those bureaucratically sanctioned courses, I also taught an introductory math camp to incoming PhD students and, along with my esteemed former colleague Pei-Hsun Hsieh, I co-taught a weekly introduction to R during the Fall 2019 semester to graduate students from sundry social sciences.\nCurrently, materials are only available for the math camp. The rest seem to have been apprehended by federal authorities from a Mar-a-Lago bathroom. A flyer from the Introduction to R course survives, however."
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-09.html",
    "href": "math-camp-pages/linear-algebra/la-09.html",
    "title": "Chapter 9: Nonsquare matrices as transformations between dimensions",
    "section": "",
    "text": "This is another short ‘footnote’ video according to the Sanderson.\nThis is all about interdimensional travel. I think you can make a fundamental bifurcation. Are there more rows in the transformation matrix than the vector, or are there fewer? In notation, you have \\(\\textbf A_{ m \\times n} \\textbf x_{n \\times 1} = \\textbf v_{m \\times 1}\\)\nSome examples\n\nm <- matrix(sample(1:9, size = 6), nrow = 3)\nm\n\n     [,1] [,2]\n[1,]    9    1\n[2,]    4    2\n[3,]    5    3\n\n\n\ni2 <- c(1, 0)\nj2 <- c(0, 1)\n\nm %*% i2\n\n     [,1]\n[1,]    9\n[2,]    4\n[3,]    5\n\nm %*% j2\n\n     [,1]\n[1,]    1\n[2,]    2\n[3,]    3\n\n\nLet’s now imagine that we take the 2D Cartesian plane and treat that as the \\(z = 0\\) “floor” of 3D space. What transformation would encode that?\nFirst, you have to realize that you simply want the following to happen, for any vector \\(\\vec x = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\) you want it to become \\(\\vec x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ 0 \\end{bmatrix}\\). Blah blah, answer is \\(\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0&0 \\end{bmatrix}\\)\nHere’s a slightly more complicated version of the last one. What if you want to make what was the 2D plane and turn it into the vertical plane? That is, imagine embedding the 2D plane in 3D space and then upending it so that it ends up perpendicular to itself?\nSomething is going wrong in my imagination. I feel like these should be perpendicular, but the dot product isn’t turning out to be perpendicular.\nWhat’s the evenly spaced gridlines interpretation of squishing n > 1 dimensional space onto the number line?\nYou imagine evenly spaced dots going along a line getting projected down onto the number line and being evenly spaced there.\nHow would you interpret \\[\\begin{bmatrix} -0.8&2.1&3.4 \\end{bmatrix}\\] as a transformation?\nCompressing down to one dimension (the number line)"
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-09.html#running-code",
    "href": "math-camp-pages/linear-algebra/la-09.html#running-code",
    "title": "Chapter 9: Nonsquare matrices as transformations between dimensions",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-08.html",
    "href": "math-camp-pages/linear-algebra/la-08.html",
    "title": "Chapter 8: Inverse matrices, column space and null space",
    "section": "",
    "text": "Gaussean elimination and row echelon form. He’s not going to cover these, but mentions that they’re important.\nWhat’s a system of equations? Most abstractly, it’s a set of \\(m\\) equations that have \\(n\\) variables (unknowns).\nExamples\n\\(x = 5\\) (1 equation and 1 variable … not really unknown – “unknowns” are just things you have to solve for)\n\\(2x - 3y = 8\\) (1 equation and 2 unknowns)\n\\(2x - 3y = 8 \\\\ x = 5\\) (now we’re cooking with gas; we’ve got a system)\n\\(2x - 3y = 8 \\\\ 1x + 0y= 5\\) (to make things line up)\n\\[ x + y + z = 3\\\\ x + z = 1\\\\  z =0\\] (ok, three equations in three unknowns)\nHow could we convert that last set of three equations into matrix vector multiplication?\n\\[\\begin{bmatrix} 1&1&1 \\\\ 1 &0&1 \\\\ 0&0&1 \\end{bmatrix}\\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 1 \\\\ 0 \\end{bmatrix}\\]\nAt this point have you talked about how matrix multiplication changes dimensions and what conformability is?\nGS says that \\(\\textbf A \\vec x = \\vec v\\) sheds light on the geometric interpretation of the problem (of solving for \\(\\vec x\\)). What does he mean by that? As we’ve done in previous lessons, you think of matrix \\(\\textbf A\\) as a transformation, and you ask, what what vector \\(\\vec x\\) will end up at \\(\\vec v\\) when transformed by \\(\\textbf A\\).\nWhat’s the notation for an inverse matrix?\n\\(\\textbf A^{-1}\\)\nWhat’s the key property of inverse matrices?\n\\(\\textbf A \\textbf A^{-1} = \\textbf I\\)\nAlso,\n\\(\\textbf A^{-1} \\textbf A = \\textbf I\\)\nIt’s important to mention that since, in general, matrix multiplication isn’t commutative.\nGrant doesn’t mention this, but both notationally and functionally, the inverse matrix with its exponent of \\(^{-1}\\) is analogous to the \\(^{-1}\\) you can slap on scalars. For example, \\(x \\cdot x^{-1} = x^1 \\cdot x^{-1} = x^{(1 - 1)} = x^0 = 1\\)\nHow is \\(A\\vec x = \\vec c\\) solved? You premultiply both sides by \\(\\textbf A^{-1}\\)\nIf \\(\\textbf A = \\begin{bmatrix} 1&1\\\\2&2 \\end{bmatrix}\\), what’s \\(\\textbf A^{-1}\\)? LOL.\nTerminology: singular\n“Rank” - number of dimensions in the output of a transformation - number of dimension in the column space - you can think of 1 - rank/max possible rank as amount of collapsing\nSo, what’s full rank? when n(C(A)) = number of columns\nWhy isn’t the answer to the previous question the number of rows?\nIf rank(A) = number of columns - 1, a line of vectors gets squished onto 0. But if rank(A) = number of columns - 2, then an entire plane gets sent by that transformation onto 0.\nThose vectors that get sent to the zero vector are referred to as the nullspace or the kernel. \\(\\text{null}(\\textbf A) = \\{x:\\textbf Ax = 0\\}\\) Get it? Vectors that end up at null.\nWhat’s the definition of column space? The set of all possible outputs of \\[\\textbf A \\vec v\\] span(a_1, a_2, , a_n)"
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-08.html#running-code",
    "href": "math-camp-pages/linear-algebra/la-08.html#running-code",
    "title": "Chapter 8: Inverse matrices, column space and null space",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-17.html",
    "href": "math-camp-pages/linear-algebra/la-17.html",
    "title": "Chapter 17: Abstract vector spaces",
    "section": "",
    "text": "Functions are another type of vector? Yes! - You can add two functions together! (f + g)(x) = f(x) + g(x) - You can scale a function by a real number! (2f)(x) = 2f(x) - Derivatives are an example of a transformation (a.k.a. linear operator) - But what does it mean for a transformation to be linear (additivity and scaling – there’s a screenshot) - He shows how this works for the derivative!\nWoah, basis functions!"
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-17.html#running-code",
    "href": "math-camp-pages/linear-algebra/la-17.html#running-code",
    "title": "Chapter 17: Abstract vector spaces",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-03.html",
    "href": "math-camp-pages/linear-algebra/la-03.html",
    "title": "Chapter 3: Linear combinations, span, and basis vectors",
    "section": "",
    "text": "https://www.youtube.com/watch?v=k7RM-ot2NWY\n\nflecha <- function(xend, yend) {\n  geom_segment(aes(\n    x = 0,\n    y = 0,\n    xend = xend,\n    yend = yend\n  ),\n  arrow = arrow(\n    angle = 20,\n    length = unit(0.1, 'inches'),\n    type = 'closed'\n  ))\n}\n\npunto <- function(x, y){\n  geom_point(\n    data = data.frame(x = x, y = y),\n    aes(x, y))\n}\n\nWhat is a linear combination of vectors?\nWhat’s the span of a set of vectors?\nStaying on the 2D plane,\nlinear combinations of two vectors can have different span different A: (Don’t know the question). If the two vectors are linearly independent, they will span the entire 2D plane. If they are on the same line, they will only span that line. If they are both the zero vector, they span … the origin.\nIf two vectors \\(\\vec v, \\vec w \\in \\mathbb R^2\\) are linearly dependent, how would indicate that given the following template \\(\\_\\vec v = \\_ \\vec w\\), where the \\(\\_\\) is a ‘blank’ to be filled in. It’s a bit tricky because you actually only need to fill in one blank, \\(\\vec v = a \\vec w\\), where \\(a\\) is the scalar that scales \\(\\vec w\\) to equal \\(\\vec v\\).\nWhat if we add a third vector? How would that change the situation?\nHow do you write a linear combination of three vectors? \\(a\\vec v + b\\vec w + c\\vec u\\)\nNow, how many elements would those vectors \\(\\vec v, \\vec w, \\vec u\\) have? A bit of a trick question!\nOk, this wasn’t covered in the video, but how would you write a linear combination of 28 vectors? Hint: You’ll use a certain tool…\n\\[\\sum_{i=1}^{28} a_i\\vec v_i,\\ i = 1, \\ldots, 28\\], where \\(a_i\\) are your 28 scalars multiplying your 28 vectors, \\(\\vec v_i\\). 28 is just a random number; hopefully you see that the idea is that now you have a way to write a combination of an arbitrary number of vectors, \\[\\sum_{i=1}^{n} a_i\\vec v_i,\\ i = 1, \\ldots, n\\] Final question\n“Given how I described a basis and given the meanings of span and linearly independent, why is the technical definition of basis the right one?”\nHere’s the technical definition, “The basis of a vector space is a set of linearly independent vectors that span the full space”"
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-03.html#running-code",
    "href": "math-camp-pages/linear-algebra/la-03.html#running-code",
    "title": "Chapter 3: Linear combinations, span, and basis vectors",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-02.html",
    "href": "math-camp-pages/linear-algebra/la-02.html",
    "title": "Chapter 2: Vectors, what even are they?",
    "section": "",
    "text": "The three perspectives on what we mean by “vector”\nLet’s ignore the mathematician’s perspective\nAn arrow inside a coordinate system\ntail on the origin – that’s unlike the physics perspective\nThis is something hinted at in the video. How would you go from the vector-as-arrow perspective to the vector-as-list-of-numbers perspective?\nThe arrow’s “tip” lies at a certain coordinate in \\(n\\)-space. That coordinate can be represented as a list of numbers.\nWhat’s the difference between \\(\\begin{pmatrix} 6 & 9 \\end{pmatrix}\\) and \\(\\begin{bmatrix} 6 \\\\ 9 \\end{bmatrix}\\)? The former is a point and the latter is a vector.\nVector addition.\nThe diagonal of the parallelogram (he doesn’t cover that – but you should)\nVector multiplication (scalar-vector multiplication)\nk > 1, 0 < k < 1 Scaling a vector, scalar\nGS says that, in LA, numbers go around scaling so much that it’s commong to use “number” and “scalar” interchangeably. Just to be clear, though the contents … maybe … IDK.\nI don’t think he mentions anything about vector notation!\nWhat does it mean for vectors to be linearly dependent?\nThese are possibly from another video: What does it mean to think of each element of a vector as a scalar? Answer: Entry \\(v_i\\) scales the corresponding \\(\\hat e_i\\) (superior notation to i, j, k – reminds me of how Laura bragged about Bogota street names) ? Something about each vector is a recipe for scaling and adding together unit vectors I think he must mean \\(\\vec v = \\sum_{i = 1}^n v_i \\hat e_i\\)\nHow does GS explain the meaning of “basis” in “basis vectors”?\nShow how, for any target vector \\(\\vec v = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix}\\), you can get there using any two linearly independent vectors. And then you can think of those combinations you generate as being the coordinates of that vector were those the two basis vectors spanning the space (it’s easy to see when the unit vectors are those ones you’re using)\nWhat’s a linear combination and whence the “linear” in “linear combination”?\nWhat’s the span of a single vector? \\(\\vec v = \\begin{bmatrix} v \\end{bmatrix}\\) \\(\\vec v = \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix}\\) \\(\\vec v = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3\\end{bmatrix}\\) (can’t visualize past that!)\nWhat’s the span of two vectors?"
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-02.html#running-code",
    "href": "math-camp-pages/linear-algebra/la-02.html#running-code",
    "title": "Chapter 2: Vectors, what even are they?",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-16.html",
    "href": "math-camp-pages/linear-algebra/la-16.html",
    "title": "Chapter 16: A quick trick for computing eigenvalues",
    "section": "",
    "text": "This was brought up at breakneck speed, but can you remember what a matrix’s characteristic polynomial is?\nHow are eigenvalues related to a matrix’s characteristic polynomial? Eigenvalues are the roots of a matrix’s characteristic polynomial.\nWhat was the trace of a matrix? If your matrix is \\[\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\], then it’s \\(a + d\\).\nOk, but most matrices are bigger than 2x2. How would you generalize the concept of a \\(\\text{tr}(\\textbf A)\\)? The sum of \\(\\text{diag}(\\textbf A)\\), or, \\[\\sum_{i=1}^n a_{ii}\\]\nHow is a matrix’s trace related to its eigenvalues? Note: there are two ways to answer, one is more straightforward, the other is more relevant to where we’re going\n\\(\\text{tr}(\\textbf A) = \\sum_{i=1}^n \\lambda_i\\). You can divide both sides by \\(n\\) to get \\(n^{-1}\\text{tr}(\\textbf A) = n^{-1}\\sum_{i=1}^n \\lambda_i = \\text{mean}(\\lambda_i)\\)\nIn the \\(\\textbf A \\in \\mathbb R^2\\) case, we have \\(a + d = \\lambda_1 + \\lambda_2\\) or \\(\\frac{a + d}2 = \\frac{\\lambda_1 + \\lambda_2}2\\)\nEnough trace for now. How is the determinant of a matrix related to its eigenvalues?\n\\[\\text{det}(\\textbf A) = \\Pi_{i=1}^n \\lambda_i\\]\nIn the \\(\\textbf A \\in \\mathbb R^2\\) case, we have \\(ad - bc = \\lambda_1 \\lambda_2\\)\nI need to go back and find out how he got that the eigenvalues are evenly spaced from … what?"
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-16.html#running-code",
    "href": "math-camp-pages/linear-algebra/la-16.html#running-code",
    "title": "Chapter 16: A quick trick for computing eigenvalues",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-14.html",
    "href": "math-camp-pages/linear-algebra/la-14.html",
    "title": "Chapter 14: Change of basis",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-14.html#running-code",
    "href": "math-camp-pages/linear-algebra/la-14.html#running-code",
    "title": "Chapter 14: Change of basis",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-15.html",
    "href": "math-camp-pages/linear-algebra/la-15.html",
    "title": "Chapter 15: Eigenvectors and eigenvalues",
    "section": "",
    "text": "Very basic question here. What has eigenvectors/eigenvalues? A: Matrices (transformations)\nJust glancing at the transformation matrix \\[\\textbf A = \\begin{bmatrix} 3 & 0 \\\\ 4 & 8 \\end{bmatrix}\\], which vectors are going to stay on the line they span after the transformation \\(\\textbf A \\boldsymbol v\\)?\nAnswer: Vectors of the form \\(\\begin{bmatrix} 0 & j\\end{bmatrix}\\)\nWhat is an eigenvector’s eigenvalue? An eigenvalue is the factor by which the eigenvector is stretched or squished. It’s the eigenvector’s associated scalar.1\nHow can you interpret the eigenvector of a 3D transformation? A: This is a bit of a trick question. If its eigenvalue is 1, then that eigenvector is the axis of rotation of the transformation. Remember that rotations in the 2D plane didn’t have (real) eigenvalues!\nWhat is the equation that unites eigenvectors with eigenvalues? \\(\\textbf A \\vec{v} = \\lambda \\vec{v}\\)\nHow do you transform \\(\\textbf A \\vec{v} = \\lambda \\vec{v}\\) so that the left-hand side is matrix-vector multiplication (the right side already is!)? You insert an identity matrix! \\(\\textbf A \\vec{v} = \\lambda \\boldsymbol I_n\\vec{v}\\)\nWhat equation do you set up to solve for \\(\\vec{v} \\text{ and } \\lambda\\)? Hint: start with the result of the previous problem.\n\\(\\textbf A \\vec{v} = \\lambda \\boldsymbol I_n\\vec{v} \\\\ \\textbf A \\vec{v} - \\lambda \\boldsymbol I_n\\vec{v} = \\vec 0 \\\\ (\\textbf A - \\lambda \\boldsymbol I_n)\\vec{v} =\\vec 0\\)\nThen, since \\(\\vec{v} \\neq \\vec 0\\), we know that \\(\\text{det}(\\textbf A - \\lambda \\boldsymbol I_n) = 0\\).\nXXX Geometrically, why do we want \\(\\text{det}(\\textbf A - \\lambda \\boldsymbol I_n) = 0\\)? ???\nWhat’s a shear? https://en.wikipedia.org/wiki/Shear_mapping\n[There’s definitely stuff missing here]\nWhat’s the “eigen interpretation” of a diagonal matrix? Its columns are where the basis vectors end up. Importantly, they end up on the line that they originally spanned. This means that the diagonal entries (scalars) are the eigenvalues (the amount by which the basis vector is scaled).\nShow/prove to your own satisfaction why it is the case that the eigenvectors of a diagonal matrix are basis vectors and their eigenvalues are the diagonal entries of the matrix.\nWhat’s an eigenbasis?\nWhen can you (not) get an eigenbasis?\nHow do you get a matrix into its eigenbasis?\nHow does an eigenbasis help you exponentiate a matrix?\nTODO: Insert Markov example\nThere’s the problem to do at the end.\nYou should probably bring up positive definite here! And also … Linear Models in Matrix Form. Check out that book’s section on positive definite.\nWhat’s the “eigen-interpretation” of a diagonal matrix? Its columns are where the basis vectors end up. Importantly, they end up on their original span, meaning that they’re eigenvectors! This makes the diagonal values their corresponding eigenvalues\nWhat’s an eigenbasis? It’s unclear, but I think it’s just a basis for the space we’re talking about where each column is … now I’m lost.\nWhen can you (not) get an eigenbasis? You need enough eigenvectors to span the full space\n(Not sure that this question is even phrased right) How do you get a matrix into its eigenbasis?\nEigenvalues are roots of a matrix’s characteristic polynomial\nHow does GS represent the idea that tr(A) = _i? \\(n^{-1} = n^{-1} \\sum \\lambda_i\\) (the mean)\n\\(det A = ad - bc = \\lambda_1 \\lambda_2\\)\nThe shortcut is \\(m \\pm \\sqrt{m^2-p}\\)\nI have no idea where this question came from: Confirm that the product of the eigenvalues of \\(\\begin{bmatrix} c&a-bi \\\\ a+bi&c \\end{bmatrix}\\) is -1 if \\(a^2 + b^2 + c^2 = 1\\)\nAt 12.20 we the homework. The mission is to prove how the determinant and trace are related to eigenvalues. Specifically, prove that \\(det A = \\lambda_1 \\lambda_2\\) and the trace fact. He gives us the quadratic ^2 - (a+d)+ (ad-bc) and says taht there’s something about these coefficients \\(-(a+d)\\) and and \\(ad-bc\\).\nExtras: What would the eigenvalues of a singular matrix be? Why?\nJustin, can you give an inkling into how eigenvalues are used in dimensionality reduction?\n\n\n\n\n\nFootnotes\n\n\nIn fact, I really think eigenvalue should have been “eigenscalar,” but that’s what they get for asking this Eigen guy to name it.↩︎"
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-15.html#running-code",
    "href": "math-camp-pages/linear-algebra/la-15.html#running-code",
    "title": "Chapter 15: Eigenvectors and eigenvalues",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-05.html",
    "href": "math-camp-pages/linear-algebra/la-05.html",
    "title": "Chapter 5: Matrix multiplication",
    "section": "",
    "text": "Linear transformations are functions with vectors as inputs and vectors as outputs\nFor previous video Why is it the case that (or what is meant by) any linear transformation can be completely described by what it does to the basis vectors? Answer: If the original vector is \\[\\vec w = \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}\\], then its coordinates after the transformation will be \\[\\begin{bmatrix} w_1 L(\\hat i) \\\\ w_2 L(\\hat j) \\end{bmatrix}\\]\nA composition is when you apply multiple transformations\nWhat’s the ‘geometric meaning’ of multiplying matrices? First multiplying by one, then multiplying by the other.\nReading right to left (and multiplication not being commutative)\nThis one’s laborious. What’s the Grant Sanderson way of notating out matrix multiplication? Use these two matrices (which, to make things spicy, contain the first eight letters of the Greek alphabet)\n\\[\\begin{bmatrix} \\alpha & \\beta \\\\ \\gamma &  \\delta  \\end{bmatrix} \\cdot \\begin{bmatrix} \\epsilon & \\zeta \\\\ \\eta &  \\theta \\end{bmatrix}\\] Answer\n\\[\\begin{bmatrix} \\alpha & \\beta \\\\ \\gamma &  \\delta  \\end{bmatrix} \\cdot \\begin{bmatrix} \\epsilon & \\zeta \\\\ \\eta &  \\theta \\end{bmatrix} = \\left [  \\epsilon \\begin{bmatrix} \\alpha \\\\ \\gamma \\end{bmatrix} + \\eta \\begin{bmatrix} \\beta \\\\ \\delta \\end{bmatrix} \\ \\ \\ \\zeta \\begin{bmatrix} \\alpha \\\\ \\gamma \\end{bmatrix} + \\theta \\begin{bmatrix} \\beta \\\\ \\delta \\end{bmatrix}\\right ]\\] What should we take from this? Make sure you’ve done the problems! https://www.3blue1brown.com/lessons/matrix-multiplication"
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-05.html#running-code",
    "href": "math-camp-pages/linear-algebra/la-05.html#running-code",
    "title": "Chapter 5: Matrix multiplication",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-11.html",
    "href": "math-camp-pages/linear-algebra/la-11.html",
    "title": "Chapter 11: Cross Products",
    "section": "",
    "text": "The initial image for this chapter is a parallelogram that two vectors create (by copying them)\nv w is the area of the parallelogram\nIt can be negative (which means that order matters)\nWhat Sanderson’s mnemonic for the sign of the cross-product? (See screenshot) He uses the fact that i j is +1, so if v w also has v to w’s right, then it’s positive as well (by analogy) And this is because v represents where i hat lands (remember, if i hat and j hat flip, then the orientation flipped and so the area is negative)\nHow is the determinant relevant to the cross-product? You put the two vectors as columns in a matrix and then compute the determinant (… but what if you had two three dimensional vectors??)\nMore perpendicular, bigger parallelogram\nav w = a(v w)\nHow does new vector’s length related to the parallelogram created by two vectors?\n\nequal to the area of the parallelogram created by two 3D vectors\nperpendicular to the parallelogram (note that these two facts don’t uniquely determine the direction of the cross-product – use the right-hand rule for that)\n\nSanderson mentions the ‘trick’ we can use to calculate the cross-product\n\\[\\text{det}\\begin{pmatrix} \\hat i & u_1 & v_1 \\\\ \\hat j & u_2 & v_2 \\\\ \\hat k & u_3 & v_3  \\end{pmatrix}\\]\nWhat would happen if instead we calculated the following:\n\\[\\text{det}\\begin{pmatrix} \\hat i & \\hat j & \\hat k \\\\ u_1 & u_2 & u_3 \\\\ v_1 & v_2 & v_3  \\end{pmatrix}\\]\nNothing would change! It’s an equally valid way to calculate the cross product. The reason \\(\\text{det} (\\textbf A) = \\text{det} (\\textbf A^T)\\)\nGiven two vector lengths (as in, imagine you have arrows of a given size, but you can point them wherever you want), how can you maximize the parallelogram they create?\n\\[0 = \\frac d{d\\theta} (|a||b|\\sin \\theta) = |a||b|\\cos \\theta\\] Now, to get \\(|a||b|\\cos \\theta = 0\\), \\(\\cos \\theta\\) has to be zero, and this happens at \\(\\pi\\) and \\(\\frac {3\\pi} 2\\)"
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-11.html#running-code",
    "href": "math-camp-pages/linear-algebra/la-11.html#running-code",
    "title": "Chapter 11: Cross Products",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-10.html",
    "href": "math-camp-pages/linear-algebra/la-10.html",
    "title": "Chapter 10: Dot products and duality",
    "section": "",
    "text": "The traditional description of the dot product – purely arithmetic. NB you need two dimensions of the same dimension\nOther names:\nWhat’s the geometric interpretation of the dot product? A: There’s a screenshot The dot product of two vectors is the length of the projection of one vector onto the length of the other vector\nWhat does it mean to project a vector onto another (geometrically)?\nGeometrically speaking, where does the sign of the dot product come from? Screen shot\nWhy does order not matter? There are some screenshots Is it the length of the projection or the length of the vector projected onto that is scaled?\nA diagonal number line example\n\ndiagonal line that goes through the origin\n\\(\\hat u\\), the unit vector that sits on the diagonal line\npoints in the original space represented their two coordinates\nproject them onto the diagonal line\nany line of evenly spaced dots will the evenly spaced on the new line – thus it’s a linear transformation!\nall this means that there exists a projection matrix , \\(A_{1 \\times 2}\\) whose two entries are where \\(\\hat i\\) and \\(\\hat j\\) land, respectively\nzooms in to make it visual\nSYMMETRTY\n\nBecause all three involved vectors are unit vectors, you can draw lines of symmetry and show that where i_hat lands on \\(\\hat u\\) is where \\(\\hat u\\) lands on i, which simply the x coordinate of u. The same logic applies to \\(\\hat j\\)\n\n\nStatement: taking the dot product with a unit vector can be interpreted as projecting the vector onto the span of the unit vector and getting its length (show how that falls out of the formula)\nWhat’s a synonym of ‘orthogonal’? -Perpendicular\nSome important dot products\ndotting a vector with itself (pythagorean logic, say the elements are a and b, then a^2 + b^c) (sum of squares; if you take off the mean then you get the observed variance)\ndotting a vector with the one’s vector\n\\(X^TX\\) can be seen as dotting all the columns together\n\\[\\text{cov}(x, y)= n^{-1} \\sum(x - \\bar x)(y - \\bar y)\\]\n\\[r = \\frac{\\sum(x - \\bar x)(y - \\bar y)}{\\sqrt{\\sum(x - \\bar x)^2\\sum(y - \\bar y)^2}} = \\frac{x'^Ty'}{|x'||y'|}\\]\nShow how you can get this from a demeaned (deviate?) matrix!"
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-10.html#running-code",
    "href": "math-camp-pages/linear-algebra/la-10.html#running-code",
    "title": "Chapter 10: Dot products and duality",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-04.html",
    "href": "math-camp-pages/linear-algebra/la-04.html",
    "title": "Chapter 4: Linear Transformations and Matrices",
    "section": "",
    "text": "A transformation is linear if it satisfies what two properties? (You should be able to state their respective meanings ‘in English’ and write them out in math notation)\nIt must a) preserve sums and b) preserve scaling. In notation, these are:\n\\[L(\\vec v + \\vec w) = L(\\vec v) + L (\\vec w) \\\\ L(s\\vec v) = s L(\\vec v)\\]\nTo briefly step down from the heights of linear algebra, imagine the function \\(f\\) that takes a number and multiplies it by three, so \\(f(x) = 3x\\). This is linear! \\(f(x + y) = f(x) + f(y)\\) and \\(f(sx) = sf(x)\\). I encourage you to test it out by substituting a number in for \\(x\\). But now let \\(g\\) be \\(g(x) = x^2\\). I encourage you to try that out and see if it’s a linear transformation.\nIn the text version of this lesson we read, “You can know that a transformation is linear if all those grid lines [of a 2D plane] which began parallel and evenly spaced remain parallel and evenly spaced (why?).” Let’s answer why? (Note that answers will vary.)\nXXX This requires quite a bit of thought, I think. Let’s just pick two parallel grid lines, x = 1 and x = -1. As vectors these are \\(\\pm \\hat i + \\alpha\\hat j\\), where \\(\\alpha\\) ranges over the entire real number line. You can XXX\nIdea, if the lines had the same slope at t = 0, then they should have it after the transformation as well. You should show that. But what does it mean for them to be evenly spaced? It seems like it means for any two points respective points whose line connecting them was perpendicular, the perpendicular line connecting them after the transformation should be a constant as well (it might be a different constant).\nOn the heels of the previous quote from the text version of the lesson, we read, “If a transformation is linear, it must also fix the origin in place (again, why?).” Let’s answer that as well.\nXXX\nLet’s say we have a transformation that takes the unit vectors \\(\\hat i, \\hat j\\) and moves them to \\(\\begin{bmatrix} 2\\\\ 2 \\end{bmatrix}\\) and \\(\\begin{bmatrix} 0\\\\ 0.5 \\end{bmatrix}\\), respectively. Where would the vector originally at \\(\\begin{bmatrix} 2\\\\ 2 \\end{bmatrix}\\) end up?\nCould you gganimate a transformation? I think so …\n\\[\\begin{bmatrix} 1 & 1\\\\ 0 & 2 \\end{bmatrix}\\] and \\[\\begin{bmatrix} 1 & 2\\\\ 0 & 1 \\end{bmatrix}\\]\nare both [insert name] kinds of transformations. Which of them compresses the vertical dimension closer to the \\(x\\)-axis?\nS shows how you’d create a \\(90°\\) counterclockwise transformation matrix. What about a \\(90°\\) clockwise transformation? What about a 45 degree rotation?\nApplication Idea Sanderson mentioned that neural networks involve linear transformations, so I thought it would be chill to cover those! But also couldn’t you do it with regression?"
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-04.html#running-code",
    "href": "math-camp-pages/linear-algebra/la-04.html#running-code",
    "title": "Chapter 4: Linear Transformations and Matrices",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-12.html",
    "href": "math-camp-pages/linear-algebra/la-12.html",
    "title": "Chapter 12: Cross products as transformations",
    "section": "",
    "text": "Plan. Define a 3D-to-1D linear transformation in terms of v and w Find its dual vector Show that this dual is \\(\\vec v \\times \\vec w\\)\nYou can imagine an incorrect extrapolation from a what a crossproduct is in 2D (though, does that really exist?) to what it is in 3D: the determinant of a matrix containing 3 vectors \\(\\vec v_i \\in \\mathbb R^3\\).\nBut you can make a change where you substitute out the first vector for the scalars (coordinates) and thus create a function that takes a 3-tuple and maps it to a number (see screenshot)\nReturns the volume of the parallelpiped determined by x, y z and v and w\nIt’s a linear function. He leaves it to us to show that this function is linear.\nNow, because it’s linear, we know that we can describe it using matrix multiplication. That is, there’s a 1 x 3 matrix that encodes it (AND there’s a dual vector).\nDamn. You’re going to have to re-watch this and understand the role of the dual vector.\n\nWay 1. The vector such that applying the transformation is the same thing as taking the dot product with that vector.\nWay 2. Geometrically, you can reason that the dual vector must be perpendicular with v and w the length equal to the area of the parallelogram spanned by spanned by the two vectors."
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-12.html#running-code",
    "href": "math-camp-pages/linear-algebra/la-12.html#running-code",
    "title": "Chapter 12: Cross products as transformations",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-06.html",
    "href": "math-camp-pages/linear-algebra/la-06.html",
    "title": "Chapter 6: Three-dimensional linear transformations",
    "section": "",
    "text": "How would you break down in the GS way this matrix multiplication from the video?\n\\[\\begin{bmatrix} 0& -2& 2\\\\ 5 &1 &5 \\\\ 1& 4&-1  \\end{bmatrix} \\cdot \\begin{bmatrix} 0&1 &2\\\\  3& 4&5 \\\\ 6& 7&  8\\end{bmatrix}\\]\nAnswer: (Remember, the jth column in the resultant composition matrix will be the entire premultiplying matrix where each of its columns is ‘weighted’ by the elements in the jth column of the postmultiplying matrix – that actually gives me an idea … FINE )\n\\[\\begin{bmatrix} a& b& c\\\\ d &e &f \\\\ g& h&i  \\end{bmatrix} \\cdot \\begin{bmatrix} i_1&j_1 &k_1\\\\  i_2& j_2&k_2 \\\\ i_3& j_3&  k_3\\end{bmatrix}\\]"
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-06.html#running-code",
    "href": "math-camp-pages/linear-algebra/la-06.html#running-code",
    "title": "Chapter 6: Three-dimensional linear transformations",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-07.html",
    "href": "math-camp-pages/linear-algebra/la-07.html",
    "title": "Chapter 7: The determinant",
    "section": "",
    "text": "What’s GS’s successive argumentation to show that the change in area for any shape will be scaled by the same amount?\nA: Unit square. Then, what happens to one square happens any square, because of the parallel and evenly spaced gridlines theorem. From that it follows then any shape (which can be approximated as a bunch of squares) will suffer the same multiplication by a factor\nWhat has determinants? Transformations (matrix)\nWhat does a determinant of zero mean? If the transformation reduces the dimensionality of space.\nNegative determinants\n‘invert the orientation of space’ Normally you’d have j hat to the left of i hat. If that changes, you’ve reversed the orientation of space. But what does that … mean. How could you operationalize this?\nAsk what the x-axis is in the screenshot of Det I took.\nParallelepiped\nRight-hand rule and left-hand rule\nIntuition.\n\\(ad - 0 \\cdot 0\\) And these generate paralellograms with height d and base a \\(ad - b \\cdot 0\\) \\(ad - 0 \\cdot d\\)"
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-07.html#running-code",
    "href": "math-camp-pages/linear-algebra/la-07.html#running-code",
    "title": "Chapter 7: The determinant",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-13.html",
    "href": "math-camp-pages/linear-algebra/la-13.html",
    "title": "Chapter 13: Cramer’s rule",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "math-camp-pages/linear-algebra/la-13.html#running-code",
    "href": "math-camp-pages/linear-algebra/la-13.html#running-code",
    "title": "Chapter 13: Cramer’s rule",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "math-camp-pages/calculus/calculus-1.html",
    "href": "math-camp-pages/calculus/calculus-1.html",
    "title": "Chapter 1: The Essence of Calculus",
    "section": "",
    "text": "Link to the YouTube video\nThis video is all about \\(\\pi r^2\\) and where it might come from. Will we discover how pi was figured out?\n\nQuestionsAnswers\n\n\n\n“Math has a tendency to reward you when you respect its ____________”\n\n\n\n\nSymmetries\n\n\n\n\nCutting a cirlce into concentric rings. You flatten them out. What shape do you get and what’s its area?\nYou’d get trapezoids, but we’ll represent them as rectangles. Their length is \\(2\\pi r_i\\) where \\(r_i\\) is the radius of the ith concentric circle and its with is \\(dr\\).\nWe’ll discover integrals, derivatives, and the fact that these two are opposites."
  },
  {
    "objectID": "math-camp-pages/calculus/calculus-11.html",
    "href": "math-camp-pages/calculus/calculus-11.html",
    "title": "Chapter 11: Taylor Series",
    "section": "",
    "text": "For each of these you should\n\nlink to youtube video\n\n\nhave two versions\n\n\nan “essentials” version that is mostly a recap\nand a ‘ad astra’ version that goes deeper\n\n\nuse in econometrics and machine learning\nnext step\n\n\n\nQ: In the video, Grant refers to Taylor polynomials as approximations of functions. Can you flesh that out? A: The function being approximated is usually a non-polynomial function and a Taylor polynomial is … a polynomial.\nQ: Abstractly, what form does a quadratic approximation take? A: \\(f(x) \\approx c_0 + c_1 x + c_2 x^2\\)\nQ: Can you then guess what forms a linear and cubic approximation will take, respectively? A: A linear approximation would be \\(f(x) \\approx c_0 + c_1 x\\) (classic \\(a + mx\\) line formula), and a cubic approximation \\(f(x) \\approx c_0 + c_1 x + c_2 x^2 + c_3 x^3\\)\nQ: Now, using summation notation can you write out the form of an nth degree approximation of \\(f(x)\\)? A: \\[f(x) \\approx \\sum_{i=0}^n c_ix^i\\]\nQ: Can you recapitulate the process that gave \\(1 - \\frac{x^2}2\\) as the approximation of \\(\\cos x\\) near 0? A: You can go to the video at 5:40 to check yourself. A: First, you have to remember that it’s a quadratic approximation, so your template is \\(\\cos x \\approx c_0 + c_1 x + c_2 x^2\\). If you had forgotten that (or don’t see that), note that \\(1 - \\frac{x^2}2 = 1 + 0 x + - \\frac 12 x\\), meaning that \\(c_0 = 1,\\ c_1 = 0,\\ c_2 = -\\frac 12\\). Your goal here is to ‘fit’ the \\(c_i\\) parameters (a fancy machine-learning inspired way of saying that you’re choosing the \\(c_i\\) values that give you the best approximation). Now, you need \\(\\cos 0 = c_0 + c_1 (0) + c_2 (0)^2 \\\\ 1 = c_0 + 0 + 0\\), so \\(c_0 = 1\\). Then, to ensure that the rate of change (slope, derivative) of the approximation is the same as the rate of change of cosine, you take the derivatives of both: $ - x = c_1 + 2 c_2 x$ and evaluate at zero: $ - = c_1 + 2 c_2 \\ 0 = c_1$. So now we have \\(c_0 = 1,\\ c_1 = 0\\). We also want their second derivatives to be the same at zero, so we take the second derivatives of both: \\(- \\cos x = 2 c_2\\) and evaluate at zero: \\(- \\cos 0 = 2 c_2 \\\\ -1 = 2c_2\\), meaning that \\(c_2 = - \\frac 12\\)\nQ: Generally, how do you make an approximation better? Can you think of any caveats? A: Increasing the degree of the polynomial makes the approximation better, usually.\nThere are two ways to understand the ‘caveat’ tag. First, remember that the value of approximations generally is that they’re simpler, so having a high degree polynomial might defeat the purpose of the approximation enterprise. The other caveat has to do with divergent series (see below).\nQ: GS has an \\(X \\rightarrow Y\\) (where \\(X\\) is the input and \\(Y\\) is the output) diagram for summarizing what Taylor polynomials are all about. From that vague clue, can you remember what it is? A: They take in derivative information at a point and give back approximations of the function’s output.\nQ: Find the linear approximation of \\(f(x) = 5 - 2x + 3x^2\\) A: As a note, if you’ve taken calculus before you’ll remember that the \\(3x^2\\) term ‘dominates’ the output as \\(x \\rightarrow \\pm \\infty\\), meaning that the linear approximation will be extremely bad (we lopped off the most relevant part!!). That’s why it’s important to occasionally remember that we’re approximating outputs for values near the expansion point, which in this case is zero. And notice that around 0, \\(3x^2\\), the term that ends up dominating, is actually ‘less relevant’ because squaring a value \\(x\\) where \\(|x| < 1\\) and when \\(|x| << 1\\), \\(|x^2| << |x|\\)\nQ: Find the quadratic approximation of \\(f(x) = 5 - 2x + 3x^2\\) A: The first thing you’d ideally have remembered is that, as your polynomial expansions get higher in degree, the lower degree part doesn’t change. So [insert previous bit] You could also notice that \\(5 - 2x + 3x^2\\) is already in the ‘template’ form of \\(c_0 + c_1 x + c_2 x^2\\). There was no work to do! The general point is that an nth degree Taylor polynomial ‘approximation’ of an nth degree polynomial function is just the original function\nQ: Here’s a harder one. Find the cubic approximation of \\(\\ln x\\) A:\nConvergence Divergence."
  },
  {
    "objectID": "math-camp-pages/calculus/calculus-12.html",
    "href": "math-camp-pages/calculus/calculus-12.html",
    "title": "Chapter 12: The Other Way to Visualize Derivatives",
    "section": "",
    "text": "for a previous video\nThe derivative of a function is a new function that for every input returns the slope at that point.\n\n\n\nhttps://www.3blue1brown.com/lessons/derivatives-and-transforms\nWhat does Grant call this generalized way of understanding derivatives? “The transformational view”\nIn what way will it be more general? It “generalize[s] calculus beyond functions whose inputs and output are simply numbers.” That is, functions whose inputs and outputs are real numbers.\nCan you complete this phrase from the video, I’d encourage you not think of this derivative-as-slope idea as being the definition of the derivative; instead think of the derivative as … ‘… as asking how sensitive is the function to tiny nudges around a given input.’ Interestingly he then goes on to show a picture of mapping numbers on a number line to their corresponding number on a different number line … seems like reals to reals!\nQ: In your own words, what’s this ‘stretch/squish’ understanding of the derivative? A: ‘If you take a small area around a point on a number line \\([x_0 -a, x_0 + a]\\), the derivative of at that point is about how much those points are being stretched (if \\(|f'(x_0)| > 1\\)) or squished (if \\(|f'(x_0)| < 1\\)) on the ’output’ number line. So, they become spread out if the function stretches inputs around \\(x_0\\) and they become contracted if the derivative is\nQ: In this numberline-to-numberline ‘transformational view’ hat does it look like to have a derivative of zero? A: It looks as if the points just stack atop each other. Note: he does x^2, you should do x^3 + 3x (wait, how does that never have a derivative of zero?!)\nWhat are the two kinds of fixed points? That is, what are their respective names and what defines them? A: Stable and unstable fixed points."
  },
  {
    "objectID": "math-camp-homepage.html",
    "href": "math-camp-homepage.html",
    "title": "3B1B Math Camp",
    "section": "",
    "text": "Below, dear reader, you’ll find the materials for a five-day intensive math camp I have in Fall 2023. Though it was imparted to incoming political science PhD students (a rather specific crowd), its fundamental nature makes it useful to anyone entering a statistics, econometrics, or machine learning program (or anyone who wants to learn the calculus and/or linear algebra necessary for such programs).\nEven before I created this, the number of math camp-y introductions to calculus and linear algebra was already vast beyond comprehension. At this point anyone who creates their own introduction is, whether they know it or not, pursuing some type of vanity project. So I decided to pursue a slightly less vain project and piggyback on Grant Sanderson’s wonderful 3Blue1Brown series of videos on calculus and linear algebra. His videos are both beautiful and profound. The only pedagogical issue is that there are neither questions nor problem sets associated with the material. Call me as old-fashioned as muddled sugar, bitters and whiskey, but it’s practice that makes perfect. So, under the ‘Content’ header below, there is a page for each of Sanderson’s videos that contains a mix of comprehension questions and problems to solve with pen and paper. As one says in this type of situation, any mistakes in the material below are my own responsibility and, in all likelihood, Grant Sanderson doesn’t even know about this material."
  },
  {
    "objectID": "math-camp-homepage.html#about",
    "href": "math-camp-homepage.html#about",
    "title": "3B1B Math Camp",
    "section": "About",
    "text": "About\nWho is this for? This is for anyone who needs to learn calculus and/or linear algebra for a statistics, econometrics, or machine learning program. The material is fundamental enough that none of it is irrelevant for any of those.\nThe specific occasion was a five-day intensive math camp I taught to incoming political science PhD students in the late summer of 2023. Because the whole idea of an intensive math camp that remediates years of quantitative training is folly, I decided that I want to create a resource that would last beyond the five days and be useful when the students were actually faced with difficult concepts in their second semester course.\nThe number of introductions to is vast and at this point anyone who creates their own introduction is, whether they know it or not, pursuing some type of vanity project. So I decided to pursue a slightly less vain project and piggyback on Grant Sanderson’s wonderful 3Blue1Brown series of videos on calculus and linear algebra."
  },
  {
    "objectID": "math-camp-homepage.html#content",
    "href": "math-camp-homepage.html#content",
    "title": "3B1B Math Camp",
    "section": "Content",
    "text": "Content\nVideos with an asterisk are those most relevant to a future practitioner of statistics.\nRight now these pages are in their beta version. This means that the notes for the 3Blue1Brown videos themselves are complete, but the statistics extensions are for the most part still outstanding.\nMost of the questions on the pages below are extremely basic. For my fellow Americans who went through mediocre middle and high schools, they’ll bring back memories of lazy “comprehension questions” handed out by teachers after a reading. Nevertheless, if you’ve ever consciously experienced the voraciousness of forgetfulness, you’ll know that even the basics aren’t immune. If you really want to learn the material, I’d recommend scheduling a reminder and going over the material again a week after the first time.1\nSo, en resumen, there are2 three sections in each of the following documents: comprehension questions reviewing the video, ‘canonical’ questions that anyone who says they’ve studied this subject would be expected to know, and a ‘going further’ section that is idiosyncratic to each video.\n\nCalculus\n\nThe essence of calculus*\nThe paradox of the derivative*\nDerivatives formulas through geometry*\nVisualizing the chain rule and product rule*\nWhat’s so special about Euler’s number \\(e\\)*\nImplicit differentiation\nLimits, L’Hopital, epsilon-delta definition\nIntegration and the fundamental theorem of calculus*\nWhat does area have to do with slope?*\nHigher order derivatives\nTaylor series*\nThe other way to visualize derivatives\n\n\n\nLinear Algebra\n\nThe essence of linear algebra*\nVectors*\nLinear combinations, span, and basis vectors*\nLinear transformations and matrices*\nMatrix multiplication*\nThree-dimensional linear transformations*\nThe determinant*\nInverse matrices, column space and null space*\nNonsquare matrices\nDot products and duality*\nCross Products\nCross products as transformations\nCramer’s rule\nChange of basis\nEigenvectors and eigenvalues*\nA quick trick for computing eigenvalues*\nAbstract vector spaces\n\n\n\nSelected Precalculus\nPrecalculus is an expansive grab bag of subjects (too many!) and it sounds inglorious (not sexy!). But you want to work with statistics, you should understand exponential and logarithmic functions much better than you probably do right now.\n\nNo materials of my own forthcoming. Just grab these dudes’ book.\n\n\n\nProbability\n\nComing … maybe? In the meantime I recommend David Morin’s Probability for the Enthusiastic Beginner. It’s maybe the best introductory book I’ve read to any topic ever."
  },
  {
    "objectID": "math-camp-homepage.html#further-resources",
    "href": "math-camp-homepage.html#further-resources",
    "title": "3B1B Math Camp",
    "section": "Further resources",
    "text": "Further resources\nFor the hurried I’d recommend\n\nJeff Gill’s Essential Mathematics for Political and Social Research\nCohen’s Linear Algebra\n\nAnd for those looking to delectate in the details, nothing beats MIT’s OpenCourse."
  },
  {
    "objectID": "math-camp-pages/calculus/calculus-8.html",
    "href": "math-camp-pages/calculus/calculus-8.html",
    "title": "Chapter 8:",
    "section": "",
    "text": "YouTube video"
  },
  {
    "objectID": "math-camp-pages/calculus/calculus-9.html",
    "href": "math-camp-pages/calculus/calculus-9.html",
    "title": "Chapter :",
    "section": "",
    "text": "YouTube video"
  },
  {
    "objectID": "math-camp-pages/calculus/calculus-4.html",
    "href": "math-camp-pages/calculus/calculus-4.html",
    "title": "Chapter :",
    "section": "",
    "text": "YouTube video"
  },
  {
    "objectID": "math-camp-pages/calculus/calculus-5.html",
    "href": "math-camp-pages/calculus/calculus-5.html",
    "title": "Chapter5 :",
    "section": "",
    "text": "YouTube video"
  },
  {
    "objectID": "math-camp-pages/calculus/calculus-7.html",
    "href": "math-camp-pages/calculus/calculus-7.html",
    "title": "Chapter 7:",
    "section": "",
    "text": "YouTube video"
  },
  {
    "objectID": "math-camp-pages/calculus/calculus-6.html",
    "href": "math-camp-pages/calculus/calculus-6.html",
    "title": "Chapter 6:",
    "section": "",
    "text": "YouTube video"
  },
  {
    "objectID": "math-camp-pages/calculus/calculus-2.html",
    "href": "math-camp-pages/calculus/calculus-2.html",
    "title": "Chapter :",
    "section": "",
    "text": "YouTube video"
  },
  {
    "objectID": "math-camp-pages/calculus/calculus-3.html",
    "href": "math-camp-pages/calculus/calculus-3.html",
    "title": "Chapter :",
    "section": "",
    "text": "YouTube video"
  },
  {
    "objectID": "math-camp-pages/calculus/calculus-10.html",
    "href": "math-camp-pages/calculus/calculus-10.html",
    "title": "Chapter :",
    "section": "",
    "text": "YouTube video"
  },
  {
    "objectID": "math-camp-homepage.html#further-linear-algebra-and-calculus-resources",
    "href": "math-camp-homepage.html#further-linear-algebra-and-calculus-resources",
    "title": "3B1B Math Camp",
    "section": "Further linear algebra and calculus resources",
    "text": "Further linear algebra and calculus resources\nImplicit in the idea of a math camp is that you’re prioritizing mere familiarity and speed over the depth and breadth that a sequence of ‘real’ courses would give. So, if you’re interested in these topics or your productivity and reputation as a researcher depends on competence in them, I encourage you to check out the resources below.\nFor the hurried I’d recommend the following two resources:\n\nJeff Gill’s Essential Mathematics for Political and Social Research covers much more than calculus and linear algebra. It has A+ examples and applications, as well.\nMike X Cohen’s Linear Algebra (Book, Udemy course)\n\nAnd for those looking to delight in the details, nothing beats MIT’s “OCW Scholar” courses. They contain everything you could want from an online course: videos of the lectures, recitation sessions, problem sets, and exams. The three most relevant for you would be:\n\nSingle Variable Calculus\nMultivariable Calculus\nLinear Algebra\n\nFor those looking a calculus textbook, I don’t think anything beats APEX’s Calculus. On the same page you can see the group has textbooks for precalculus, matrix algebra, and other topics as well."
  }
]