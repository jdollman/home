---
title: "Chapter 4: Linear Transformations and Matrices"
format: html
---

A transformation is linear if it satisfies what two properties? (You should be able to state their respective meanings 'in English' and write them out in math notation)

It must a) preserve sums and b) preserve scaling. In notation, these are:

$$L(\vec v + \vec w) = L(\vec v) + L (\vec w) \\ L(s\vec v) = s L(\vec v)$$

To briefly step down from the heights of linear algebra, imagine the function $f$ that takes a number and multiplies it by three, so $f(x) = 3x$. This is linear! $f(x + y) = f(x) + f(y)$ and $f(sx) = sf(x)$. I encourage you to test it out by substituting a number in for $x$. But now let $g$ be $g(x) = x^2$. I encourage you to try that out and see if it's a linear transformation.

In [the text version of this lesson](https://www.3blue1brown.com/lessons/linear-transformations) we read, "You can know that a transformation is linear if all those grid lines [of a 2D plane] which began parallel and evenly spaced remain parallel and evenly spaced (why?)." Let's answer *why?* (Note that answers will vary.)

XXX
This requires quite a bit of thought, I think. Let's just pick two parallel grid lines, x = 1 and x = -1. As vectors these are $\pm \hat i + \alpha\hat j$, where $\alpha$ ranges over the entire real number line. You can XXX

Idea, if the lines had the same slope at t = 0, then they should have it after the transformation as well. You should show that. But what does it mean for them to be evenly spaced? It seems like it means for any two points respective points whose line connecting them was perpendicular, the perpendicular line connecting them after the transformation should be a constant as well (it might be a different constant).

On the heels of the previous quote from the text version of the lesson, we read, "If a transformation is linear, it must also fix the origin in place (again, why?)." Let's answer that as well.

XXX

Let's say we have a transformation that takes the unit vectors $\hat i, \hat j$ and moves them to $\begin{bmatrix} 2\\ 2 \end{bmatrix}$ and $\begin{bmatrix} 0\\ 0.5 \end{bmatrix}$, respectively. Where would the vector originally at $\begin{bmatrix} 2\\ 2 \end{bmatrix}$ end up?

Could you gganimate a transformation? I think so ...

$$\begin{bmatrix} 1 & 1\\ 0 & 2 \end{bmatrix}$$ and $$\begin{bmatrix} 1 & 2\\ 0 & 1 \end{bmatrix}$$

are both [insert name] kinds of transformations. Which of them compresses the vertical dimension closer to the $x$-axis?

S shows how you'd create a $90°$ counterclockwise transformation matrix. What about a $90°$ clockwise transformation? What about a 45 degree rotation?

Application Idea
Sanderson mentioned that neural networks involve linear transformations, so I thought it would be chill to cover those! But also couldn't you do it with regression?