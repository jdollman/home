{
  "hash": "7ce18fe159e7227048c013238be7068c",
  "result": {
    "markdown": "---\ntitle: \"Sentiment Analysis Case Study (I)\"\ndescription: \"Ezra Klein Show\"\nauthor: Justin Dollman\ndate: 03-02-2024\ndate-modified: \"09/02/2024\"\ncategories: [R, sentiment analysis, text analysis, Ezra Klein]\nformat:\n    html:\n      code-fold: true\n      code-tools: true\ndraft: true\n---\n\n\n# Introduction\n\nIn this case study I'll imagine one were given a corpus consisting of the transcripts of a podcast (in this case, [The Ezra Klein Show](https://www.nytimes.com/column/ezra-klein-podcast)) and asked to perform a sentiment analysis. What can you find out? Which methods work well, which do not? \n\nA couple notes before we begin.\n\nThis is going to be a from-square-one case study, where I start with methods so bad that I wouldn't even bother doing them in any real application just to show you how bad they are.^[\"Why do them at all?\" you sensibly ask. Because I fear that most people who \"learn\" sentiment analysis learn just the bad way. \"A little learning is a dangerous thing,\" as [the saying](https://en.wiktionary.org/wiki/a_little_knowledge_is_a_dangerous_thing) goes.] After covering basic use of dictionary-based methods, I'm going to introduce one rule-based strategy and then finish off with a deep learning approach.\n\nMy second prefatory note is related to the first. This case study is a little anachronistic. This is how one would have performed a sentiment analysis in the mid-to-late 2010s. As we can't help but know, Natural Language Processing (NLP) has made huge strides in the 2020s. Future posts will cover those. This post is then somewhat of historical interest. It's also me working through these things I learned back in the day one last time.\n\nThe last note is a technical note. The code here is the code you *would* need to run the analyses *if* you had the transcript data. I cannot provide that data, but you hopefully have some other text data you want to analyze! Assuming you've cleaned it, you should basically be able to copy this code, drop your data in, and be off to the races! \n\nAlright, let's begin.\n\n# The Data, The Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# # text libraries\n# library(sentimentr)\n# library(vader)\nlibrary(tidytext)\n# \n# # others\n# library(ggtext)\nlibrary(tidyverse)\n# library(ggupset)\n# \n# # data\n# #ezra <- read_csv()\n# political_words <- read_csv('../../Utils/Political Words/500_political_words.csv')\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# ezra %>% \n#   slice_sample(n = 20)\n```\n:::\n\n\nHow many episodes? An important consideration in any text analysis is, How much text is there? In this case, we have around 220 episodes from [first] to [last] where each episode contains around XXX words. That's enough data for doing basic descriptive analyses. In future case studies the size of the data will be much, much larger.^[In case you're wondering what too little text would be for doing analyses, I once tutored a student who was making wordclouds of individual Janis Joplin songs. It was wild.]\n\nFollowing the principles set forth in Silge and Robinson's *Text Mining in R*, we'll get the data in a 'tidy' format where we take the unstructured utterances in the `text` column above, tokenize them, and then have one row per token.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ezra_tidy <- \n#   ezra %>% \n#   unnest_tokens(token, text)\n```\n:::\n\n\n\n# Dictionaries\n\nOut of the box, the `tidytext` package gives us four sentiment dictionaries: AFINN, BING, NRC, and Loughran and McDonald's dictionary for financial sentiment. Given this application, we'll ignore the last one.^[If you're interested in learning more about these dictionaries, see this page.] Let's briefly familiarize ourselves with them.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_sentiments(lexicon = 'afinn') %>% \n  slice_sample(n = 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 × 2\n   word       value\n   <chr>      <dbl>\n 1 heartfelt      3\n 2 easy           1\n 3 hardy          2\n 4 disbelieve    -2\n 5 accuses       -2\n 6 imposed       -1\n 7 loving         2\n 8 validates      1\n 9 coerced       -2\n10 importance     2\n11 inspire        2\n12 regretful     -2\n13 delayed       -1\n14 douche        -3\n15 postponing    -1\n16 contagious    -1\n17 starved       -2\n18 assets         2\n19 shrew         -4\n20 doom          -2\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nget_sentiments(lexicon = 'bing') %>% \n  slice_sample(n = 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 × 2\n   word        sentiment\n   <chr>       <chr>    \n 1 moribund    negative \n 2 shrug       negative \n 3 stigmatize  negative \n 4 bluring     negative \n 5 inestimably positive \n 6 washed-out  negative \n 7 muddy       negative \n 8 unsafe      negative \n 9 excuse      negative \n10 uneven      negative \n11 reproachful negative \n12 hoodium     negative \n13 peculiar    negative \n14 refusal     negative \n15 harmless    positive \n16 bloodshed   negative \n17 prejudicial negative \n18 unclean     negative \n19 impolite    negative \n20 misbehave   negative \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nget_sentiments(lexicon = 'nrc') %>% \n  slice_sample(n = 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 × 2\n   word          sentiment\n   <chr>         <chr>    \n 1 wanting       sadness  \n 2 radioactive   negative \n 3 music         sadness  \n 4 chronicle     trust    \n 5 trepidation   surprise \n 6 obliterated   fear     \n 7 accessible    positive \n 8 destroying    anger    \n 9 embarrassment sadness  \n10 foreign       negative \n11 inquiry       positive \n12 brag          negative \n13 scandal       fear     \n14 bastion       positive \n15 paranoia      negative \n16 bankruptcy    fear     \n17 commerce      trust    \n18 unfulfilled   sadness  \n19 clarify       positive \n20 bulldog       positive \n```\n:::\n:::\n\n\nIf you didn't just scroll past these lines of code and output, you'll notice that, despite the structural similarity of being key-value two-column tibbles, the three lexica are fundamentally different. AFINN maps words to values between -5 and 5, inclusive. BING maps words to 'positive' and 'negative' (which you'd probably turn into -1 and 1 for analyses). NRC maps words to a panoply of different sentiments, among which you can find positive and negative. There's a lot that can be said about these dictionaries, and if you're doing to perform a 'serious' sentiment analysis, part an absolute minimum due diligence would be for you to know the ins and outs of these and other dictionaries.^[As mentioned above, you can check out my one-stop dictionary explainer.] For the purposes of this case study, however, we're just going to move on to another big problem\n\n\n::: {.cell}\n\n```{.r .cell-code}\nafinn <- get_sentiments(lexicon = 'afinn')\nbing <- get_sentiments(lexicon = 'bing')\nnrc_emotions <- filter(get_sentiments(lexicon = 'nrc'), \n                    !(sentiment %in% c('positive', 'negative')))\nnrc_polar <- filter(get_sentiments(lexicon = 'nrc'), \n                    sentiment %in% c('positive', 'negative'))\n```\n:::\n\n\n## Sentimental about Politics\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# political_words %>% \n#   left_join(rename(bing, bing = sentiment), join_by('token' == 'word')) %>% \n#   left_join(rename(afinn, afinn = value), join_by('token' == 'word')) %>%\n#   left_join(rename(nrc_polar, nrc = sentiment), join_by('token' == 'word')) %>% \n#   mutate(afinn = ifelse(afinn > 0, 'positive', 'negative')) %>% \n#   pivot_longer(bing:nrc, names_to = 'lexicon', values_to = 'polarity') %>% \n#   filter(!is.na(polarity)) %>% \n#   slice_max(order_by = n, n = 15, by = lexicon) %>%\n#   mutate(lexicon = str_to_upper(lexicon),\n#          polarity = str_to_title(polarity),\n#          token = str_to_title(token)) %>% \n#   ggplot(aes(x = n, y = reorder_within(token, n, within = lexicon), fill = polarity)) +\n#   geom_col() +\n#   tidytext::scale_y_reordered() +\n#   facet_wrap(~lexicon, ncol = 1, scales = 'free') +\n#   theme_minimal() +\n#   theme(\n#     legend.position = 'none',\n#     strip.text = element_text(face = 'bold', size = rel(.7),\n#                               margin = margin(.08,0,.08,0, \"lines\")),\n#     strip.background = element_rect(color = 'gray20', fill = 'grey92')\n#   ) +\n#   labs(\n#     y = '',\n#     x = '',\n#     fill = '',\n#     title = \"Hey! That's not a sentiment word! That's a neutral word!\",\n#     subtitle = \"Words tidytext's main dictionaries classify as positive or negative, but aren't\"\n#   )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# ezra_tidy %>% \n#   inner_join(get_sentiments('bing'), by = join_by(\"token\" == \"word\")) %>% \n#   count(token, sentiment, name = 'count') %>% \n#   slice_max(order_by = count, n = 30) %>% \n#   ggplot(aes(count, y = fct_reorder(token, count), fill = sentiment)) +\n#   geom_col() +\n#   scale_x_log10(expand = expansion(0, 0),\n#     labels = scales::label_comma()) +\n#   labs(\n#     x = 'Token Count',\n#     y = '',\n#     title = 'Top 30 Words Most Important for Ezra\\'s Sentiment Scores ',\n#     fill = 'Polarity',\n#     caption = 'Caveat lector: x-axis is on log scale'\n#   ) +\n#   theme_minimal() +\n#   theme(legend.position = c(.9, .2),\n#         panel.grid.major.y = element_blank())\n```\n:::\n\n\nDoes this get cleared up after removing political words from the dictionaries? Also, how big of a difference does this make to 'takeaways'? A distribution of changes for each dictionary!\n\n## Handling Negation\n\nThere's another issue\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ezra_tidy_bi <- \n#   ezra %>% \n#   unnest_ngrams(token, text, n = 2L, n_min = 2)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# ezra_tidy_bi %>% \n#   select(speaker, token)\n# \n# ezra_tidy_bi %>% \n#   select(speaker, token) %>% \n#   separate_wider_delim(col = token, delim = ' ', names = c('a', 'b'))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# lexicon::hash_valence_shifters %>%\n#   as_tibble() %>% \n#   filter(y == 1)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# negations <- as_tibble(hash_valence_shifters) %>% filter(y == 1) %>% pull(x)\n# \n# ezra_bi_sentiment <- \n#   ezra_tidy_bi %>% \n#   select(speaker, token) %>% \n#   separate_wider_delim(col = token, delim = ' ', names = c('a', 'b')) %>% \n# #   inner_join(get_sentiments('bing'), by = join_by(\"b\" == \"word\")) %>%\n#   mutate(negation = ifelse(a %in% negations, TRUE, FALSE))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# c(sum(ezra_bi_sentiment$negation), mean(ezra_bi_sentiment$negation))\n```\n:::\n\n\nAs a last note, it might make sense to expand the window before the word to more than just the immediately preceding word. Following what we did above, it would be straightforward to extract *tri*grams and flip the target word's valence if either one of the two preceding words were negative. But at that point we might as well embrace the fact that we're now doing *rule-based* sentiment analysis and move on to the next section.\n\n# The Rules-Based Approach\n\nLet me start by saying that this header implies that rules-based approaches are *not* dictionary-based. False, fake news. They *are* dictionary-based, they just deploy the dictionaries more intelligently. \"More intelligently\" here refers to taking account of words' context beyond a simple, \"Was this word immediately preceded by a negation?\"^[Technically, inverting the sentiment score if a word is preceded by a negation *is* a rule-based approach, but it's so simple that [insert \"C'mon, *testa di cazzo* Italian hand gesture].] In this tutorial I'm going to cover one rule-based sentiment scoring procedure, `sentimentr` (CITATION).\n\n## `sentimentr`\n\n`sentimentr` was developed by Tyler Rinker, prolific author of `R` packages, to be ??? trade off against speed^[In the next post I'll measure how long the various methods take to tag texts of different lengths to see exactly what kinds of gains we're talking about].\nRinker describes `sentimentr` as an \"augmented dictionary lookup.\"\n\nWithout going into the rules themselves, I'll just mention here that Rinker's `sentimentr` handles of four \"valence shifters\": \n\n1. negators (e.g., *not* good; old story)\n2. intensifiers (*really* good)\n3. 'downtoners' (*somewhat* good)\n4. adversative conjunctions (It's good, *but* I didn't like it.)\n\n# Deep Learning\n\nHave a photo of Chace Crawford\n\nRecursive Neural Network (not a *recurrent* neural network)\n\nRecursivity was singled out as an important feature of human language since at least Noam Chomsky's ???\n\nA tour through the treebank. \nBefore reading further, try to answer a few questions.\n\nHow are numbers at the nodes calculated?\nWhich number is the overall sentiment of the tree?\n\nIt still makes mistakes (see image)\n\nRichard Socher et al. 2013^[\"Dick Socher?! I don't even know her!\"]\n\n\n# Conclusion\n\nThere's a phrase in the XXX episode, \"four inch nail embedded in your face.\" For most of us, that would be an extemely painful and negative thing. But not for a sentiment dictionary. This is a good example of needing to think through whether concepts are adequately captured by individual words. In this case, no. \n\n## Methodological Takeaways\n\n## Substantive Takeaways\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}