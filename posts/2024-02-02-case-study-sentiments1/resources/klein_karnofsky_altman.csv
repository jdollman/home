date,speaker,text
2021-10-05,Ezra Klein,"Over the past— I don't know — five, six years, I've been very influenced by the effective altruism movement. On one level, effective altruism is simple. It asks, how do we do the most good we can with the money and the resources we have? That turns out to be, one, a deceptively difficult question and, two, weirdly, one that we don't ask all that often, one that oftentimes you think people are asking and they are not. But the difficult parts are maybe more interesting. How do you measure the most good? What about when you think something is good, but it cannot really be measured? Who defines good? Who verifies impact? How do you judge the value of, say, supporting art against the value of building housing for the poor?Effective altruism has roots in the academy. Philosophers like Toby Ord and Will MacAskill and Peter Singer, they've been central in creating the movement. And importantly, they're central in the way the movement thinks and reasons. The culture of effective altruism, in my experience — and this is both its best and worst quality, in a way — can feel like a philosophy grad seminar that never ends. By that, I mean it delights in taking the logic of its questions as far as it will go. It's unafraid, even ecstatic, to follow answers that strike others as very strange or unintuitive, sometimes even cruel. It's always, always questioning its own assumptions and everyone else's. It can, in my view, sometimes be performatively cold or logical in a way that's actually quite narrow about human flourishing. But as I've said, I have learned a lot from these thinkers. What's interesting to me is that effective altruists tend to come out on two very different ends of what you might end up worrying about. Some take the analysis and end up worrying a lot about very provable, very, very well defined interventions, like distributing malarial bed nets, because those are the interventions we have studied with the best research. We've done randomized controlled trials. And we know they save lives cheaply. And then others come out obsessed with much more speculative threats, like, say, killer artificial intelligence because if you start running the thought experiments, if you assign a moral weight to the future and all the potential human beings in the future, anything that could end humanity tomorrow really demands our attention today. If you can improve the likelihood of countless generations even a little bit, that really nets out to a big impact on the future. No one represents the dual and sometimes warring impulses of effective altruism better, to me, than Holden Karnofsky. Holden was one of the co-founders of GiveWell, which measures the effectiveness of different charities and recommends the ones it is most confident can save lives cheaply. But then he spun out of that to found Open Philanthropy, which is more on the speculative side of effective altruism, the thought experiment side. But because he's actually giving away money and making grants and shaping sectors and commissioning research, he and his team have to be pretty serious about how they approach questions that are not always considered seriously. They don't even always sound serious when they are spoken aloud. So there is a practicality, even a groundedness, to his analysis, even when it ends up in very, very strange places. And more recently, Holden has been going to stranger places still. He launched a blog called Cold Takes. And on it, he's making the case, in piece after piece, that we live in maybe the most important century humanity will ever have, the most important we have had or will have, that the future could be wildly unlike the past. It is real mind expanding stuff. And this episode — warning — will go to some very strange places. This is not our normal fare. But I'd urge you to follow it there. There's a concept Holden and I talk about here — worldview diversification, the practice of recognizing that we're not always sure how we should look at the present or the future. And we should keep a number of different possibilities, some of them even outlandish, alive in our minds. We should even keep them alive in our actions, rather than trying, as we so often do, to choose between them. This is something I try to do on the show. You can think of each episode as a little exercise in worldview diversification. And I try, as you know, to stay away from the choosing, except when we really have to. But it's particularly something we do in this episode. Is Holden right about some of the more wild predictions we discuss at the end? I don't know. On some level, I have a lot of trouble believing it, but I do think it's good to stretch the predictions you're willing to entertain, if only to help you see the present more clearly. As always, my email: ezrakleinshow@nytimes. com. Holden Karnofsky, welcome to the show."
2021-10-05,Holden Karnofsky,Thanks for having me.
2021-10-05,Ezra Klein,"I've wanted to do this a long time, man."
2021-10-05,Holden Karnofsky,Cool.
2021-10-05,Ezra Klein,"It's a minute in the making. All right, I want to begin at the beginning for you, before Cold Takes, before Open Phil, back in the GiveWell days. Tell me the story of why you started GiveWell."
2021-10-05,Holden Karnofsky,"Sure. So I was a few years out of college, and I wanted to give to charity. And I had the immediate thought that maybe I could find a website that would just tell me where to give to charity to sort of get the best deal. I think you could maybe think of what I wanted as a Wirecutter for charities, although there was no Wirecutter then, either. What I wanted was to kind of help as many people as I could with the money that I was giving. And I tried to find this, and I couldn't. And my coworker, Elie Hassenfeld, and I were in the same boat on this. We were going through the same journey together. And we both decided to create the website that we wish existed, which is GiveWell, which is just public recommendations, telling people which charities can help their money help the most people possible."
2021-10-05,Ezra Klein,So I looked for things like this in this period. And I found things that at least purported to be this. People might be familiar with Charity Navigator. There's a lot of cross-charity budget comparisons. Why weren't those what you were looking for?
2021-10-05,Holden Karnofsky,"Sure, so at the time we were doing this was around 2006, 2007. And the obsession in the charity evaluation world was around the overhead ratio, or how much of a charity's budget does it spend on the so-called programs versus the so-called overhead. This is an appealing thing in that you have to report these numbers to the I.R.S., so you can get this number for any charity. But it really doesn't seem all that relevant to me. For example, if you have a charity and you decide to pay your top talent more or you get better information technology, and now you're doing way more good and helping way more people, is that overhead? Is that somehow a waste of money? Is that, somehow, you did the wrong thing? And so, we wanted something different. What we wanted was this question of, what charity is going to help the most people per dollar that I spend?And a crucial difference is that we were not in a place where we had a charity we already liked, and we wanted to look it up and make sure it was legit or that it wasn't fraudulent. We wanted something different. We wanted recommendations of which charities out of all the many you could do the most good. And so kind of like how the Wirecutter will actually kind of tell you what five products to maybe buy, instead of you look up a product and they tell you if it's going to break."
2021-10-05,Ezra Klein,"On a note, the Wirecutter is now a New York Times company, and I've not paid Holden to make all these Wirecutter analogies. But getting at this, one of the reasons there was an interest in things like overhead spending is it is cross-comparable across charities. As you mentioned, that's a number reported to the I.R.S. There are a lot of charities. They do very different things. Some of them work on international poverty. Some of them work on public health. Some of them give money to ballet. How do you begin to narrow down and test which ones do the most good for the least money?"
2021-10-05,Holden Karnofsky,"Yeah, so we started with some criteria. We wanted to find charities that were proven, cost-effective and scalable. Proven means that there's strong evidence that what they're doing helps people. Cost-effective means that they're helping a lot of people per dollar. And scalable means that there's room for more funding. It means that if you give more money, more people will be helped. So we're not just talking about a charity that did something great once and wants more money now. Those criteria will not capture everything good. And I imagine we'll get to this, that I've later branched out into other kinds of giving that don't have necessarily all those criteria. But when you have those criteria, if you look at any charity and you ask whether it's really helping people, you've got an academic literature to review. Does education interventions, do those help children learn better? Do health interventions help people live longer? And what you can do is you can start looking for the programs that are the most proven and the most cost effective. And then you can find charities that do them. So that's a way to narrow the field. And one of the things that we learned early in GiveWell, which is a lesson that's kind of stuck with me throughout my journey to where I am now, is that if you want to do the most good, in some ways, the worst place you can start is your neighborhood, your friends, your country, because if you look at the whole population, in this case, with GiveWell, there's just a lot of people living in countries that are extremely poor by U.S. standards, and your money can go a lot further there. You can help a lot more people with the same money if you're willing to broaden your moral circle and include more people in it."
2021-10-05,Ezra Klein,"Talk me through this tangibly. What is the first charity you find where there is really, really good evidence that it can help people for very little money? Where do you find the charity? And where do you find the evidence?"
2021-10-05,Holden Karnofsky,"I could talk about the literal first we ever recommended, but instead, I'll talk about the first where it really started to come together, that we felt we were nailing these criteria. So that was the Against Malaria Foundation. And what they do is they basically distribute insecticide-treated bed nets to help reduce the burden of malaria. Now, insecticide-treated bed nets cost about $5. They often cover two people. Sometimes they cover one person. They last for several years. And they can kill mosquitoes and block mosquitoes so that people don't get malaria. There's a large number of very rigorous, randomized controlled studies on bed nets. And if you look at the effect size, it looks like they are helping people very cost effectively in the sense of — we estimate that there's a death averted for every few thousand dollars that you can spend distributing bed nets, which is kind of incredible and has been very hard to beat that number at any point during GiveWell's journey. So, that's the intervention: that's distributing bed nets. And then what we found is we've looked at several organizations that distribute insecticide-treated bed nets. And one of them was really tracking the whole process. They were providing documentation that the bed nets were actually handed out. They were providing shipping information of the bed nets arriving. And over time, they would add more data on going back and surveying people if they were still using the bed nets. So now you can put together this whole case. We have very strong evidence base. You have an amazing deal, in some sense, where a lot of people get help for each dollar you spend. It's very hard to do that much good helping people in the U.S. with donations. And then you have this organization that is really repeatedly carrying it out and that is providing all the information we need. And so it becomes this very exciting way to spend your money."
2021-10-05,Ezra Klein,"So I want to hold on this idea that giving to anti-malaria charities could save a life for a few thousand dollars, or specifically, according to the GiveWell website, $3,000 to $5,000. How does that compare to more conventional causes that people may be familiar with, like disaster relief or donating to a soup kitchen?"
2021-10-05,Holden Karnofsky,"It's about the best we've been able to find, in some sense. We've looked high and low for ways to help people for small amounts of money. Some listeners might be a little confused when they hear $3,000 to $5,000 because they might have heard, well, I saw a charity that can save a life for $0.20. But at GiveWell, it's always been about rigorously investigating the numbers and subjecting them to all kinds of scrutiny and analysis. And when you really go hard on the analysis and you ask that the number be real, $0.20 to save a life is not something that I think you really have the opportunity to do with your giving. And $3,000 to $5,000, it's just empirically, it's been — I mean, I think that's just incredible compared to most things that are out there."
2021-10-05,Ezra Klein,What are some of the other numbers you've come to when you've looked at other methods?
2021-10-05,Holden Karnofsky,"A lot of times, we don't put a number on it so much as we just put a bound on it, where we'll say, hey, the evidence here is not very strong. The intervention itself is very expensive. It might be like thousands of dollars just to put a person in a program that might or might not be helping them at all. And so, a lot of times, it's more a matter of saying, this doesn't have a strong enough case, or it's just intuitively too expensive. It's not going to match that other figure."
2021-10-05,Ezra Klein,"So one of the critiques of GiveWell, the GiveWell model, is that, to use the old line about economics, it only looks under the light, right? It looks for its keys, but only where the lamppost is. Because there are a lot of things that could be good in the world, and they don't have a very — it is hard to run a randomized controlled trial of them, because they only happen once, or they are effecting something more diffuse. So what was the thinking behind demanding this very high level of empirical proof, which, on the one hand, can say, here's where my dollar is going, but on the other hand, you might say, yeah, but the leverage on that dollar is smaller because I'm only trying to save the one life as opposed to influence something, say, 10,000 or a million by averting a war or changing civil service or whatever it might be?"
2021-10-05,Holden Karnofsky,"Right, right. I have a bunch of thoughts on this. The original thinking was just very pragmatic. It's just a start-up saying, let's do what we can do. I think sometimes, you do want to start under the light. If there's a big area under the light, well, if your keys happen to be there, you're going to find them a lot more quickly. Maybe you should start there. So I think in some ways, it's not always such a bad thing to do. Another intuition we had is just that a lot of things that people try to do to help people are just very speculative. They're often based on having certain feelings about the world or feeling like certain things kind of just feel supportive to do. And we thought that things that are really supported by evidence, where you can really drill down and see how much money's going there, might actually be systematically better because they are kind of — they're optimized in a different way. Or another way you could think of this is the less you know about some intervention, the more you might expect that it's going to just be the average thing you can do. And when you can create a strong evidential case that your money is doing incredible things, that might actually be better than other options. The final thing I'll say, though, is that I, at least, have branched out a lot since then, so I now run a different organization or co-C.E.O. run a different organization called Open Philanthropy that does not have the same requirement that everything be totally proven based on evidence. And an interesting thing is that we've been looking for things that are better than GiveWell's top charities. And it's been really hard. It's really been surprisingly hard, yeah."
2021-10-05,Ezra Klein,I'm going to hold you there because we're moving to Open Phil. But I want to do this a little bit more slowly.
2021-10-05,Holden Karnofsky,Sure.
2021-10-05,Ezra Klein,"You're co-running GiveWell with Elie. It's going well. GiveWell became a big player on the block very quickly. I've given a lot of my money through GiveWell over the past five, 10 years. What makes you decide to split off and start a new organization?"
2021-10-05,Holden Karnofsky,"Well, we met Cari Tuna and Dustin Moskovitz. Dustin is one of the co-founders of Facebook and also Asana. And they were trying to give away their fortune in a way that would help the most people possible. And we just felt that a different approach might be called for. When you have a public website making recommendations to anyone and everyone versus working with one family that's giving away a huge amount of money, the second one kind of starts to put new options on the table. And so, Open Philanthropy still recommends a lot of donations to give those type of charities. And Cari and Dustin still give a lot of money there. But there have been other things that may be higher risk or may be more in the mode of what I call hits-based giving, which is that if you can get the occasional success that is a really huge win, that might make up for a lot of donations that don't quite have the effects they wanted to. And so, that's what caused us to kind of pivot. And we started Open Philanthropy as a project within GiveWell, and it eventually spun out."
2021-10-05,Ezra Klein,"Before you begin Open Philanthropy, you undertake this big study of the history of the philanthropy space."
2021-10-05,Holden Karnofsky,That's right.
2021-10-05,Ezra Klein,Tell me a bit about that study and what you learned from it.
2021-10-05,Holden Karnofsky,"When we started working with Cari and Dustin, I wanted to understand a little bit more about how big philanthropies in the past, what they had accomplished. And I didn't know what I'd find. I wouldn't have been surprised if I learned that, actually, they'd never accomplished anything, and we should go back to doing the simplest stuff we can, because it's that hard to help people. But that's not what I learned. What I learned is that there have been incredibly impressive successes from philanthropy that I think rank up there with the most important events for human welfare in the last 100 years or so. We actually at Open Philanthropy, we've now named about half our conference rooms — each one is named after a philanthropic success story."
2021-10-05,Ezra Klein,So what are they named?
2021-10-05,Holden Karnofsky,"One of my favorites is called Green Revolution. And that was when the Rockefeller Foundation funded Norman Borlaug and others to research improving crop yields in Mexico. If I'd been around at the time, I doubt I would have said, oh, improving crop yields in Mexico, that's going to be the biggest success that's ever had by philanthropy. But it may have been because this is generally now credited with kicking off the Green Revolution, where a bunch of countries went from importing to exporting food. And it's credited with saving a billion people from starvation. Norman Borlaug ended up winning the Nobel Peace Prize because it turns out that this was a very scalable improvement in agricultural productivity. So once they had these crops, they could just breed them anywhere. Agricultural productivity took off in a lot of poor countries. And that kickstarted all this economic growth. Another one of my favorites that I'll just throw in is the pill, the common oral contraceptive for birth control. The work was funded by a feminist philanthropist named Katharine McCormick. And at the time, this is the kind of thing that wasn't going to get funded by the government because it was controversial. And in fact, they weren't able to advertise the pill as birth control originally. Instead, the warning label was the advertisement. They had to put on a warning label that it could prevent pregnancy. So it was an example of philanthropy being ahead of the curve, doing something that was controversial. But they ended up with something that was transformative and was a huge moment for feminism and human welfare because they were willing to be a little controversial like that."
2021-10-05,Ezra Klein,"One thing I noticed about both of the case studies you used there as examples of huge wins is they're technologies. The Green Revolution are new kinds of crops. The pill is a medication. I don't think most philanthropy is about seeding and staking new technological development. I'm not saying none is — I, in fact, I have a friend who's working on that kind of thing right now — but not most of it. So is that a problem? Is that one of the places where Open Phil begins to diverge a view that you should actually be doing product development through philanthropy?"
2021-10-05,Holden Karnofsky,"Technology is not something I would say is neglected by philanthropy. You're probably right that most philanthropy is not technology philanthropy. There is a lot of philanthropy in science, especially today. It's become a very fashionable thing to put money into. But I would say it does reflect this idea that if you want to do really incredible things that have massive scale and help tons of people, having innovation, having new technologies that can be copied and used freely by anyone, is a great way to get leverage. It's a great way to just have really big effects. And if you don't have any theory in your philanthropy of how you're getting massive leverage and affecting huge numbers of people, then you may be better off with the most straightforward, cost-effective, proven stuff."
2021-10-05,Ezra Klein,"Well, let's talk about how to put that kind of theory into practice. Give me some examples of what you end up funding through hits-based giving."
2021-10-05,Holden Karnofsky,"I mean, one example is just we're the largest funder in the world of farm animal welfare. That's both attempts to develop alternative foods that can reduce meat consumption, but also corporate campaigns to try and put pressure for better treatment of animals. Over the last few years, basically, every major grocer and fast food company in the U.S. has pledged to go cage free. We've been funding a lot of the work that led to that that was already going when we came, but we've tried to speed it up. And we've also taken that work global and been funding a lot of corporate campaigns globally. And so, while we are a fairly large philanthropy, we're not the largest, but we're the largest funder of that work because that's one of these things where I think we might look back hundreds of years from now and say, that was the greatest moral issue of our time. That was this unacceptable treatment of these creatures we've now decided are kindred creatures that we should care about. But at this time, this is just not an issue that really tends to fire many people up. And so, we're doing something others won't do. And I believe there's been a lot of impact and there's been a lot of difference made because we're willing to go into kind of a weird cause. Some other examples, I think we were the first major institutional funder of the YIMBY movement. This is the attempt to advocate for less restrictions on building housing to make housing more affordable. And this is, again, just something that was kind of new and weird and has now become a nationwide movement. And that was not how it was when we started funding it. We funded macroeconomic stabilization policy. So this is a bit of a wonky one, but how does the Federal Reserve prioritize full employment versus controlling inflation? We believe that this is one of the most important things in the whole world for the welfare and bargaining power of the working class. And yet, it's an issue that people often ignore. They think of it as a technocratic issue. Whatever, the experts at the Federal Reserve will decide what to do. They don't see it as an area for philanthropy. And I think we are one of the only philanthropists in that area when we came in and still now. But I think it's tremendously important and we funded a lot of analysis and even advocacy on how to trade off full employment and controlling inflation. A final example is we've had a biosecurity and pandemic preparedness program since about 2015. And I'm certainly not going to say I've been happy with the preparation response for COVID. But I think it could have been worse. And I think the organizations that have played important roles, by the time we all knew about COVID, it was too late to come in and support them for years and help them be in a solid position and build up a deep bench in a lot of expertise. And it was back when that was kind of an unusual cause for philanthropy to be in that we were supporting all that work."
2021-10-05,Ezra Klein,"Let me ask about a couple of these, though. Macroeconomic stabilization is an interesting one because one way of asking that question is, why do you think you all understand macroeconomic stabilization better than the Fed and others? You're a young organization. You're a young guy. You're not an economist, nor most of the people who work for you. A lot of people worked on this for a long time. You're coming in and saying there's a huge untapped opportunity here. I would understand for a very explicitly political organization to come in and say, oh, we think the Fed has gotten it wrong. We want more full employment. But what is the difference you have convinced yourself you can make on it?"
2021-10-05,Holden Karnofsky,"Well, it's important to understand the general structure of Open Philanthropy is that we consider our expertise in finding causes that are important, neglected and tractable. Tractable means there's something for us to do. And so we try and find the right problems to work on. That's what we consider our comparative advantage. And then when we are doing work, we are hiring and we are funding experts. And so, this is not about Holden going and learning all about macroeconomic policy and then going and explaining to the Federal Reserve that they've got it wrong. That's not what happened. We funded groups that have their own expertise, that are part of the debate going on. There are experts on both sides. But we funded a particular set of values that says full employment is very important if you kind of value all people equally and you care a lot about how the working class is doing and what their bargaining power is. And historically, the Federal Reserve has often had a bit of an obsession with controlling inflation that may be very related to their professional incentives. And so we do have a point of view on when there's a debate among experts, which ones are taking the position they're taking, because that's what you would do if you were valuing everyone and trying to help everyone the most, versus which you're taking position for some other reason. So we didn't roll our own macroeconomic policy insights. We funded experts, we funded think tanks. But we do have a point of view on what kind of values should be driving that expertise."
2021-10-05,Ezra Klein,"I think something striking about that list is the sheer diversity of things you all fund. Not only in terms of causes but categories of causes. And this gets to what I think of as one of the most interesting things Open Philanthropy does, which is the way you intentionally divide up your giving portfolio into buckets based on really different ethical, arguably even metaphysical, assumptions. So tell me about worldview diversification."
2021-10-05,Holden Karnofsky,"I need to start with the broader debate that worldview diversification is a part of. At Open Philanthropy, we like to consider very hardcore theoretical arguments, try to pull the insight from them, and then do our compromising after that. And so, there is a case to be made that if you're trying to do something to help people and you're choosing between different things you might spend money on to help people, you need to be able to give a consistent conversion ratio between any two things. So let's say you might spend money distributing bed nets to fight malaria. You might spend money getting children treated for intestinal parasites. And you might think that the bed nets are twice as valuable as the dewormings. Or you might think they're five times as valuable or half as valuable or 1/5 or 100 times as valuable or 1/100. But there has to be some consistent number for valuing the two. And there is an argument that if you're not doing it that way, it's kind of a tell that you're being a feel-good donor, that you're making yourself feel good by doing a little bit of everything, instead of focusing your giving on others, on being other-centered, focusing on the impact of your actions on others, which you can get from there to an argument that you should have these consistent ratios. So with that backdrop in mind, we're sitting here trying to spend money to do as much good as possible. And someone will come to us with an argument that says, hey, there are so many animals being horribly mistreated on factory farms and you can help them so cheaply that even if you value animals at 1 percent as valuable as humans to help, that implies you should put all your money into helping animals. On the other hand, if you value them less than that, let's say you value them a millionth as much, you should put none of your money into helping animals and just completely ignore what's going on factory farms, even though a small amount of your budget could be transformative. So that's a weird state to be in. And then, there's an argument that goes, but even more than that — and this idea is called long-termism — if you can do things that can help all of the future generations, for example, by reducing the odds that humanity goes extinct. Then you're hoping even more people. And that could be some ridiculous comic number that a trillion, trillion, trillion, trillion, trillion lives or something like that. And it leaves you in this really weird conundrum, where you're kind of choosing between being all in on one thing and all in on another thing. And Open Philanthropy just doesn't want to be the kind of organization that does that, that lands there. And so we divide our giving into different buckets. And each bucket will kind of take a different worldview or will act on a different ethical framework. So there is bucket of money that is kind of deliberately acting as though it takes the farm animal point really seriously, as though it believes what a lot of animal advocates believe, which is that we'll look back someday and say, this was a huge moral error. We should have cared much more about animals than we do. Suffering is suffering. And this whole way we treat this enormous amount of animals on factory farms is an enormously bigger deal than anyone today is acting like it is. And then there'll be another bucket of money that says, animals? That's not what we're doing. We're trying to help humans. And so you have these two buckets of money that have different philosophies and are following it down different paths. And that just stops us from being the kind of organization that has stuck with one framework, stuck with one kind of activity."
2021-10-05,Ezra Klein,"Before we move on, I want to unpack this a little bit more. So let's focus in on animals for a minute. You alluded to the fact that even if you assign a very low moral worth to animals or to their suffering, 1 percent or 0.1 percent of that of a human, that it ends up adding up to quite a lot. Can you run through that math for me and its implications?"
2021-10-05,Holden Karnofsky,"Well, the math would be that — I mentioned before that if you're distributing insecticide treated bed nets, you might avert the death of someone from malaria for a few thousand dollars, which is pretty amazing. And it's going to be very hard to find better than that when you're funding charities that help humans. However, with the farm animal work, for example, the cage free pledges, we kind of estimated that you're getting several chickens out of a cage for their entire lives for every $1 that you spend. And so this is not an exact equivalence, but if you start to try to put numbers side by side, you do get to this point where you say, yeah, if you value a chicken 1 percent as much as a human, you really are doing a lot more good by funding these corporate campaigns than even by funding the bed nets. And that's better than most things you can do to help humans. Well, then, the question is, OK, but do I value chickens 1 percent as much as humans? 0.1 percent? 0.01 percent? How do you know that?And one answer is we don't. We have absolutely no idea. The entire question of what is it that we're going to think 100,000 years from now about how we should have been treating chickens in this time, that's just a hard thing to know. I sometimes call this the problem of applied ethics, where I'm sitting here, trying to decide how to spend money or how to spend scarce resources. And if I follow the moral norms of my time, based on history, it looks like a really good chance that future people will look back on me as a moral monster. But one way of thinking, just to come back to the chickens question, one way of thinking about it is just to say, well, if we have no idea, maybe there's a decent chance that we'll actually decide we had this all wrong, and we should care about chickens just as much as humans. Or maybe we should care about them more because humans have more psychological defense mechanisms for dealing with pain. We may have slower internal clocks. A minute to us might feel like several minutes to a chicken. So if you have no idea where things are going, then you may want to account for that uncertainty, and you may want to hedge your bets and say, if we have a chance to help absurd numbers of chickens, maybe we will look back and say, actually, that was an incredibly important thing to be doing."
2021-10-05,Ezra Klein,"I want to note something here because I think it's both an important point substantively but also in what you do. So I'm vegan. Except for some lab-grown chicken meat, I've not eaten chicken in 10, 15 years now — quite a long time. And yet, even I sit here, when you're saying, should we value a chicken 1 percent as much as a human, I'm like, ooh, I don't like that. To your point about what our ethical frameworks of the time do and that possibly an open-field comparative advantage is being willing to consider things that we are taught even to feel a little bit repulsive considering, how do you think about those moments? How do you think about the backlash that can come? How do you think about when maybe the mores of a time have something to tell you within them, that maybe you shouldn't be worrying about chicken when there are this many people starving across the world? How do you think about that set of questions?"
2021-10-05,Holden Karnofsky,"I think it's a tough balancing act because on one hand, I believe there are approaches to ethics that do have a decent chance of getting you a more principled answer that's more likely to hold up a long time from now. But at the same time, I agree with you that even though following the norms of your time is certainly not a safe thing to do and has led to a lot of horrible things in the past, I'm definitely nervous to do things that are too out of line with what the rest of the world is doing and thinking. And so we compromise. And that comes back to the idea of worldview diversification. So I think if Open Philanthropy were to declare, here's the value on chickens versus humans, and therefore, all the money is going to farm animal welfare, I would not like that. That would make me uncomfortable. And we haven't done that. And on the other hand, let's say you can spend 10 percent of your budget and be the largest funder of farm animal welfare in the world and be completely transformative. And in that world where we look back, that potential hypothetical future world where we look back and said, gosh, we had this all wrong — we should have really cared about chickens — you were the biggest funder, are you going to leave that opportunity on the table? And that's where worldview diversification comes in, where it says, we should take opportunities to do enormous amounts of good, according to a plausible ethical framework. And that's not the same thing as being a fanatic and saying, I figured it all out. I've done the math. I know what's up. Because that's not something I think."
2021-10-05,Ezra Klein,"I'm struck by that. I really like worldview diversification as a way of thinking about things. And I think it's also relevant as an individual practice. Something I see in my travels around the world, the internet, is people are very intent. Even if they would not say they are 100 percent confident in their worldview, their political ideology, their whatever, they are really interested in making it dominant against all comers. So, just tell me a bit about organizationally, intellectually, the discipline of maintaining a certain level of agnosticism between worldviews whose differences you can't really answer."
2021-10-05,Holden Karnofsky,"So one of my obsessions is applied epistemology, which is like just having good systems for figuring out what your beliefs are in kind of an overwhelming flow of information that is today's world. And I think one of the tools that some people use for it that I find really powerful and I'm going to write about is what I call the Bayesian mind-set, which is this idea that when you're uncertain about something, you can always portray your uncertainty as a number. And you can portray it as a probability. There's thought experiments. There's tools for doing this. You can say, instead of something is true or false, that it's 30 percent. And you can look back later and you can see if things that you said were 30 percent likely come through 30 percent of the time. I think this is a very powerful framework. And using it can often get you out of the headspace of believing that things are true or false and just having degrees of belief in everything and often taking something very seriously, even when you think it probably won't happen, just because it's important enough and it has a high enough probability that it deserves your attention. And on the other hand, I think this framework sometimes can take people back into a state of fanaticism, where you might say, hey, here's something that would be a really huge deal. And it's at least 1 percent likely. So that means it should be the only thing I think about. It should be my obsession. It's like the examples I was giving before. And that, I think, just lands you in a similarly dogmatic place. And so, Open Philanthropy is kind of operating two levels of uncertainty. It's often using this Bayesian mindset. But when the Bayesian mindset brings you to this implication that you'll have to be all in on one thing or another, we'll say no to that, too. And then we'll just go to another level of diversification. And we'll have different buckets with different philosophies on the world."
2021-10-05,Ezra Klein,"I want to pick up on the fanaticism component. And I'm not accusing anybody here of fanaticism. But one of my critiques of the effective altruist world is that it can get very obsessed by that conversion number you were talking about a minute ago. And in particular, I think, it's a culture as it has matured a bit more. There's now an aesthetic, sometimes, of being willing to take the most hard-hearted logic experiment seriously and show that you're the real effective altruist because even though it sounds like a kind of terrible thing to do, you ran the math, and it's not. And the way I'll put this is, Will MacAskill, who's a philosopher and was a founder of the effective altruist movement, used to have this thought experiment where there's a building on fire. And there's a family in one room who could die. And then there's another room — or I think it was, actually, an attached garage or something — that has a bunch of very expensive art in it. What do you save?And the point of the experiment originally was you should, of course, save the family. And he was making the meta point that many people are donating to museums, instead of to malarial bed nets. I think now, a lot of effective altruists would answer it the other way, because the point is, well, if that art is worth $500,000 and you can turn that $500,000 into x number of malarial bed nets, that saves more than five lives. And so, of course, you need to do that. And I think that gets you into pretty dangerous territory. But I'm curious how you think about those questions."
2021-10-05,Holden Karnofsky,"I do agree that there can be this vibe coming out of when you read stuff in the effective altruist circles that kind of feels like it's doing this. It kind of feels like it's trying to be as weird as possible. It's being completely hardcore, uncompromising, wanting to use one consistent ethical framework wherever the heck it takes you. That's not really something I believe in. It's not something that Open Philanthropy or most of the people that I interact with as effective altruists tend to believe in. And so, what I believe in doing and what I like to do is to really deeply understand theoretical frameworks that can offer insight, that can open my mind, that I think give me the best shot I'm ever going to have at being ahead of the curve on ethics, at being someone whose decisions look good in hindsight instead of just following the norms of my time, which might look horrible and monstrous in hindsight. But I have limits to everything. Most of the people I know have limits to everything, and I do think that is how effective altruists usually behave in practice and certainly how I think they should."
2021-10-05,Ezra Klein,"What do you think the limit of that actual thought experiment is, of the just convert lives into money? You can save x number of lives for x number of money. And so if you get more money by getting the money as opposed to saving the lives, you should do it."
2021-10-05,Holden Karnofsky,"I think there's a lot of problems with that argument. And I could sort of go into them. So there's things about setting norms. There's things about following rules so that you don't want to be the kind of person who is constantly behaving in strange, unexpected ways and screwing over people around you because you've got this strange mathematical framework that's going on. So I think there's a bunch of things that are wrong with running in and saving the painting. But I think I also just want to endorse the meta principle of just saying, it's OK to have a limit. It's OK to stop. It's a reflective equilibrium game. So what I try to do is I try to entertain these rigorous philosophical frameworks. And sometimes it leads to me really changing my mind about something by really reflecting on, hey, if I did have to have a number on caring about animals versus caring about humans, what would it be?And just thinking about that, I've just kind of come around to thinking, I don't know what the number is, but I know that the way animals are treated on factory farms is just inexcusable. And it's just brought my attention to that. So I land on a lot of things that I end up being glad I thought about. And I think it helps widen my thinking, open my mind, make me more able to have unconventional thoughts. But it's also OK to just draw a line. I think it's OK to look at this art thing and say, that's too much. I'm not convinced. I'm not going there. And that's something I do every day."
2021-10-05,Ezra Klein,"We've been talking a lot here about how to value animals, but the other big worldview here is long-termism, which has to do with valuing future human lives. So tell me more about that worldview."
2021-10-05,Holden Karnofsky,"So the basic idea of that worldview is that if you can do something today that affects all of the future generations, then you have helped a truly mind numbing number of people. We don't know how many people. We don't know how many people will live in the future. But it could be an extremely large number. Most things that we can do today are not the kind of thing that we have any reason to believe will still matter a billion years from now. But some of them could be. Climate change could be. Climate change is an example of something that could really, in theory, could imaginatively knock humanity off course forever. And causing it to be less likely that this happens could be the kind of thing that matters for every future generation. And so, long-termism says there are all these people who don't have any voice today in the actions we're taking that affect them. And so, why don't we take the actions that will still matter a billion years from now? Because they'll have affected that many people, and maybe that's the way to do the most good."
2021-10-05,Ezra Klein,"So one of the critiques of long-termism is it quickly gets you into this kind of mathematical moral blackmail, where, well, if you say, because in the future, human beings could spread throughout the galaxies and there could be a trillion of us, over and over again, there could be a trillion of us, so if you give the future human lives 1 percent of the weight of a current human life or 0.1 percent, sort of anything that makes that future more likely to happen is just an astonishingly good investment that outweighs anything you can do for people today. How do you think about that?"
2021-10-05,Holden Karnofsky,"I have a few ways of thinking about this. One way comes back to worldview diversification again. So what we aren't trying to do is find the one master framework that is the one thing. What we are trying to do is find things we can do that may turn out to be hugely ahead of the curve that may turn out to be a really big deal, that may turn out to be the best money we've ever spent. I think long-termism definitely checks that box. And so, Open Philanthropy is never going to be an organization that is exclusively long-termist. But we do put a lot of money into it. And the way you phrased it is one way of phrasing it. But if we also just phrase it another way and we say, why don't we try to focus our actions and our efforts on the things that might still matter a billion years from today, why don't we try to do the things that optimize for having the best future we can, for bequeathing the best thing we can to the future generations, for having the best overall story of humanity that we possibly can, having a healthy society, a society that makes good decisions, a society that's equitable and inclusive? I don't know. I don't think that sounds nearly so crazy. And I think if you were to walk around all day, this is something I've increasingly been trying to do myself because we've been doing division of labor at Open Philanthropy, and I've been focusing more on long-termism. But if you just walk around all day, just thinking, well, what is it that's the best for helping the world be a good place a billion years from now? I think you end up doing a lot of really super reasonable things and maybe paying a lot of attention to things that should be getting more attention today."
2021-10-05,Ezra Klein,"Well, talk me through how even think of what the long-term in long-termism is because a critique we might have of just the way human beings are right now — I don't think we're typically doing things to make 24 hours from now the best it could possibly be, right? We're pretty far off of an ideal policy. So then, one version of long-termism is, let's just think about our children's world. And another is, let's think about 150 years from now, which is pretty far in the future. But I'd feel more confident predicting trends out 150 years, knowing I'll get some of them wrong. You then begin talking about a million years, a billion years."
2021-10-05,Holden Karnofsky,"A billion years, yeah."
2021-10-05,Ezra Klein,"What is long-termism to you? Because the further you go out, obviously, the more the uncertainty begins to bite. So how do you define it? And then how do you work within that uncertainty?"
2021-10-05,Holden Karnofsky,"From a values perspective, I think that we should be caring about the whole future. But you raise an important point, which is the big obstacle to doing long-termist work is knowledge. And it's very hard to predict the future. It's very hard to identify actions that will matter that long from now. And so the vast majority of ideas we have, we probably will be wrong. And we'll probably be overconfident about what will actually matter a million years, a billion years, whatever. So this is a huge challenge. And I think it's one of the downsides of being a long-termist and one of the reasons that I haven't put my whole life into it and never will. But that doesn't mean that we're totally helpless. It doesn't mean that we should just throw up our hands and say, let's just optimize for the next one year because that's the same as optimizing for the next billion years. And climate change is an example of that, where if you're always focused on the next year, you might say, let's burn more fossil fuels because that makes us all better off today. But it's not some radical state of ignorance. It's not some giant unknown question whether climate change is going to make the long run future better or worse and whether it has the potential to make it a lot worse for a very long time. It's actually a real possibility. And so, I think as a long-termist, to do it well, you have to have taste and judgment. And you have to know when you don't know something, which is almost always, and when you might be on to something that actually could matter for that period of time. That's not an easy thing to do. And it's a pretty young idea. So I don't think we're nailing it, but I think someone should be trying it."
2021-10-05,Ezra Klein,"So I know that you started out in a place that's, frankly, more like where I started out or maybe even where I am on these questions, which is a bit more critical of the idea of long-termism, more critical of the appeal of long-termism within the philanthropic circles you run in. But recently, you've put a lot more emphasis in it. In Open Phil's 2018 update on cost prioritization, you wrote, ""We'll probably recommend that a cluster of long-termist buckets collectively receive the largest allocation, at least 50 percent of all available capital.""Now, I know those numbers are subject to change. But it speaks to the fact that long-termism is an area you decide to place a lot more emphasis on in recent years. So tell me a bit about your trajectory on that. How did you become more persuaded there? And what did you become persuaded of?"
2021-10-05,Holden Karnofsky,"Sure. So as co-C.E.O. of Open Philanthropy, my job is to try to get ahead of the curve. My job is to look for ideas that are not only important but also neglected. And so, I'm always looking for what could be the next big thing that could matter for a ton of people that's not getting enough attention. And I deliberately seek out people and ideas that can introduce me to things that might do that. And so, through this, I have encountered the idea that this century, the 21st century, could be the most important century of all time for humanity. And a primary way that might come about is the development of A.I. systems that could cause a dramatic acceleration in science and technology, such that if you were to imagine a radical sci-fi future, a technologically advanced utopia, dystopia or anything between or even maybe a world that's not run by humans at all — that's run by AIs with their own non-human compatible objectives, which we can get to — a lot of people think that kind of long-run future is possible, but the right kind of A.I. could bring it very soon, could bring it this century. And once you think that you could be in that kind of century, now the timescales have collapsed. Instead of trying to make predictions about a billion years from now, we're trying to make predictions about the specific things that could happen in the next few decades that could matter for hundreds of years, thousands of years, billions of years. And so, when I first encountered this idea, I think it all just sounded too wild and too out there. And I really kind of mostly stuck to the work I was doing. But again, it's my job not to be too dismissive of ideas that could be extremely important and extremely neglected. So, over the years, I've come to take it more seriously. And in particular, Open Philanthropy has had a team really focused for the last several years on taking this thesis about A.I. and the most important century and poking every angle at it, looking for the weak points, trying to figure out whether this is really plausible. And we've gotten to the point where I'm not going to say that this is something I know. I'm not going to say this is something that's going to happen. But I am going to say it's a serious possibility that I think deserves a lot more attention than it's getting. And the most recent kind of change for me is, I've been trying to get my thoughts straight. And so, I've started my own blog called Cold Takes, where I'm trying to lay out the case that we're in the most important century as simply as I can. And more generally, have been noticing that I've been taking on more unconventional views, more views that are important to what we're doing, but that are not widely held. And I think it's important for me to be writing up those views in a clear and simple way in public, not only to help get clear in my own head about what I believe but also — because if I'm wrong, I want it to be easier for other people to encounter what I'm saying and to show me how I'm wrong. So this is a project that I'm on now, is taking these ideas that could be astronomically important that I've started to take quite seriously and continue poking at them by kind of putting them out there."
2021-10-05,Ezra Klein,"Well, let's talk about that project. We'll go deeper into these questions around A.I. in a minute. But first, I want to step back and talk about the big picture view of your— I always feel like you need drums and horns when you say this — Most Important Century series. One of the things you're really arguing there is that the future could be profoundly unlike the past in a way that it's true that 2021 is unlike 1900, but it's a lot like 1900. It's still human beings running around. A lot of things are recognizable, a lot of the same religions. You had Democrats and Republicans in 1900. There's a lot happening then that was quite similar. But you're arguing here that 2300 could be basically unrecognizable as a world. So give me the basic case for that, the case for why the future might be wildly different than the past."
2021-10-05,Holden Karnofsky,"This is a big thing that has held me back from taking the Most Important Century idea seriously for many years, is that it just — if it's true, it implies we live in this wild, unusual time. And something I've learned is that if you just look at our time in full historical context, there's so many reasons other than A.I. to think that we do live in a wild time. So I think it's helpful to just kind of situate it in context. First, there's the past. So the universe is more than 10 billion years old. Life on Earth is more than 3 billion years old. The whole thing of a species that's creating its own technology at all, even stone tools. That's millions of years old. So that's millions versus billions. That's the blink of an eye on galactic timescales. And then human history is also just very packed into the recent past. So I think almost any metric you look at — economic growth, population, major technological milestones — more has happened in the past few hundred years than in the previous several hundred thousand or several million years. And that kind of points to this idea that there is a sort of acceleration that has occurred. Things have moved faster and faster. And if you simply project that acceleration forward and you say the acceleration continues or it's paused right now, but it comes back, in some ways, things going so crazy and the next few decades being more eventful than everything that came before is a continuation of a trend, not a breaking of it. Then, if you look to the future, I think there's other interesting observations about what a weird time we're in. Today's level of economic growth is a few percent a year. That doesn't feel that crazy to people. But it is not only an incredibly high level by historical standards, it's a level that doesn't look like it can go on forever. So if you try to project out thousands of years of growth of even 2 percent a year, it kind of looks like you've run out of atoms in the galaxy. You just can't do it. And so something has to change."
2021-10-05,Ezra Klein,"Hold there for a minute because I know the math on this, but it's very unintuitive. So, as I remember the calculation you run, 2 percent growth a year for 10,000 years will get you to a quantity that you basically can't run with the materials of the galaxy. But 2 percent doesn't seem that big."
2021-10-05,Holden Karnofsky,"Yeah, I mean, the specific quantity is at some multiple of the number of atoms in the galaxy, and there's very good reasons to think we would not be able to get even close to the edges of the galaxy in that time, because you're just constrained by the speed of light, if nothing else. So, yeah, 2 percent, I mean, it's exponential growth. And if you just plot it out 10,000 years, exponential growth tends to be very unintuitive. Accelerating growth is even more unintuitive and even more explosive. And that is something we've seen accelerating growth or what's called super exponential growth. We've seen it in the past. And if we see it in the future, yeah, I mean, things go to the moon very quickly, and it's very unintuitive. But I do believe it's something that could happen."
2021-10-05,Ezra Klein,But why this century? What makes this century so important?
2021-10-05,Holden Karnofsky,"So, three basic points. Point one is that the long-run future could be just radically unfamiliar. It could be a radical utopia, dystopia, anything in between. Point two is that the long-run future could become the near-term future if the right kind of A.I. is developed to accelerate science and technology dramatically. And point three is that that kind of A.I. looks more likely than not this century. And then the bonus point four is that when you put the three together, a natural reaction is that this implies we're in a very special time. And it sounds too wild to be true. But point four is that if you step out and look at our place in history, it looks like we're in a very weird and wild time for many reasons that have nothing to do with A.I. And so we should be ready for anything."
2021-10-05,Ezra Klein,"I think actually just the idea of acceleration here is unintuitive. I think if you know the basic growth story, it's been fairly steady 2 percent growth globally for some time. People hear a lot about great stagnation. They hear about wage stagnation. It's been very hard for a lot of countries to break out of middle income traps. It doesn't intuitively feel like we are in an era of globally accelerating growth. So, walk me through the accelerationist argument."
2021-10-05,Holden Karnofsky,"Yeah, I think we're not in a period of globally accelerating growth. And I don't think we necessarily will be again. We could just end up with a world that stagnates, where growth slows, like you're saying. The case for accelerationism would be — so the basic idea is, if you look at most standard economic growth models, there's this potential for a feedback loop. This is something that can happen. It's not something I think is happening today. But you could have a feedback loop where every time you get more resources, more food, whatever, that leads to more people. More people have more ideas. More ideas leads to innovation, and therefore, more resources. So you get resources, people, ideas, resources, people, ideas, and you get a feedback loop. And that causes accelerating growth that can be this very explosive dynamic. And it looks reasonably likely that this has described periods of economic history. The people refer to the Malthusian dynamic, where people didn't get much richer, even though they were improving productivity, because the additional resources would just go into more people. And so, you see that pattern. And what happened, what changed a couple hundred years ago or so is called the demographic transition where it stopped being the case that more resources meant more people. And now, of course, when people have a lot of wealth or have a lot of resources, they tend to just be richer. It doesn't cause them to have more children. And so —"
2021-10-05,Ezra Klein,It does the opposite in fact. They have fewer children.
2021-10-05,Holden Karnofsky,"Exactly. And so if that's the dynamic we're in, then, yeah, you would, by default, expect that we will get stagnation. And I think that's a serious possibility that our long-run future looks like economic growth has to slow dramatically. I think there's a lot of reasons to think that is our default. The question is, is there a way to get that same function provided by people, provided by something else, such as A.I., that can be straightforwardly produced by more resources. So today, we have A.I. systems that are cool. They can beat people at chess. They can transcribe audio. What they're not doing is they're not doing that innovation part of the feedback loop. They're not creating new technologies autonomously. They're not advancing science and technology on their own because there's such limits to what they could do. And the question is, if that changes, if AI is developed that could be as good as humans — doesn't have to be better — at sort of pushing science and technology forward, then you get the feedback loopback back. And then you could get an explosion."
2021-10-05,Ezra Klein,"So there are a number of things I want to question in this or describe more. But I want to first start at what I think will be the natural objection, which is something you didn't spend time on there. In a population-resources-ideas feedback loop, resources is a part of this. So I think it's gotten the worst name recently. There is a broadly held view — I hold it — we talked about climate change already in the show— that we've had a lot of growth that is using up a lot of resources in a way that, for all the good it's done, has also done a lot of harm. It's put us in a very dangerous climate situation. It has led to a tremendous amount of extinction of other species. The idea that you would just accelerate growth from there, well, with what resources? Oftentimes, growth does not create resources. It consumes them, right? We're consuming fossil fuels that are finite on this planet, et cetera. How do you think about the resource constraint? Or do you not see it as a constraint? Talk about that piece before we go into what would happen if you blew up the rest of the loop."
2021-10-05,Holden Karnofsky,"First, I just want to be clear that I'm not talking about this possibility as this is exciting, and we got to do it. I'm talking about it as this is something that might happen, for better or worse. And I think it could be very good or very bad. But in terms of resources, so humans right now are the only thing that sort of have ideas, that sort of create new technologies that advance science. And humans have a lot of needs. And there's a lot of resources for humans that are really hard to replace. There's only one planet that we really know of where it seems pretty doable for humans to exist for a long time. And once you have a different kind of technology fulfilling that part of the feedback loop, that kind of technology does not need all the same resources that humans have. What do you need in order to run more A.I.s? You need more computers. Well, you need some things for that. You need metal. You need electricity. You need cooling. But there's not that much that you need to run more computers. And actually, all the things you need are very abundant in space. And all the things you need to get to space, I think, can be built with those sort of limited set of resources. So I think that's kind of the whole idea, is that it's a lot easier to build more A.I.s than it is to build more humans. And so this loop could quickly get out of control if that dynamic changes."
2021-10-05,Ezra Klein,"I want to put a pin in there's a lot of metal in space because I think it's actually an important part of your vision, a lot of other people's visions that we should come back to in a second. But in terms of here, this is always a question I have about the AI and locking tremendous economic growth question, which is, oftentimes, the constraint on an idea improving people's lives is material in some way or another. So, for instance, there is a lot we could do with better analysis leading to better drug innovation. But actually, a huge constraint in drug innovation is you need to run a lot of trials on human beings. You need to actually test things in the real world. It slows everything down. It's a big deal. It's not clear to me, in the first approximation, where A.I. helps on that or how much it actually unlocks. Or in terms of things people might want, houses take wood. Cars need to be built. It is true that the A.I.s themselves would not need a lot of resources. But in order to get a huge amount of pressure on the growth number, which is measuring things the economy actually produces and people consume, a lot of this would have to be built. It's not going to all be digital t-shirts in the metaverse. And so, how does A.I. create more resources, as opposed to even potentially disastrously using them up? I mean, should I look at resources as fixed, and more growth will just consume them faster? Or should I look at them as like a pie you can expand? How do you think about that part of the equation?"
2021-10-05,Holden Karnofsky,"I think my first answer to the question is that if you assume that you can't translate these A.I. resources into human resources, it doesn't change the fact that we're looking at an enormously consequential development. So if you have this kind of dramatic feedback loop and a dramatic increase in sort of the reach of our world, of our planet's artifacts, if there could be sort of this growing population of A.I.s that is expanding throughout space, that's a very high stakes situation for humans. If it turns out there's no way to convert all of that wealth into wealth that makes humans' lives better, well, that's really bad news. It doesn't mean that this is a nothing burger, though. It kind of might mean this is really bad news. So when I talk about a giant acceleration in science and technological advancement, that doesn't have to be an acceleration in medicine. But it's an acceleration in some sort of general potency, some sort of resources, some sort of ability to make something happen. And the less compatible that is with humans, the worse. Then it sort of becomes up to us. I mean, if this happened, could we harness it in a way that it was to humans' benefit? Could we take a very large supply of sort of digital minds or A.I.s having ideas and use that to make the world better? Or is it just going to turn into this sort of runaway train? And I think that's a question for us."
2021-10-05,Ezra Klein,"Well, give me a concrete example. What's an example of a way, if we unlocked this boundary we have on creativity and innovation, right? You have 5 billion A.I. minds coming up with cool ideas all the time. When you think about how that could lead growth to go vertical, what are the kinds of things you're thinking might emerge? What problems that we have might they solve? I mean, it's all fine to talk about growth and GDP and A.I.s, but paint the sci-fi utopia or dystopia for me. Let's get tangible."
2021-10-05,Holden Karnofsky,"Sure, I'll start a little bit more tangible, and then I'll get a little bit more out there. So for the little more tangible, we can talk about energy. Energy is definitely something that if you had a lot of minds and you had a lot of resources, you should be able to get a lot of energy. You should be able to get it very cheap. And you should be able to get it in a way that isn't necessarily involving any greenhouse gas emissions or anything like that because there's just plenty of energy to be had if you had big enough and good enough solar panels. That's something that I think could absolutely come out of this loop. Then if we talk about health, I mean, you might be right that health is always going to be bottlenecked by human trials. But if you're able to do enough simulations, if you're able to have enough minds on the problem, you really could come up with a very large number of candidate drugs at once. You might have to wait a few years for the clinical trials. But it's sort of the sky's the limit in terms of how much you could improve health. And so, that's the answer number one, is health and energy. Well, I mean, that's an awful lot. We could go through other parts of the economy, but dramatic changes in health, lifespan and energy would certainly be a big deal. Now I'm going to go more to the wilder end of the spectrum and the more speculative end because I think it's important for people to not have their aperture too narrow and not be stuck on things that feel like today, when we could be looking at dramatic changes. So I do want to talk about the more radical end of things. It's very important not to have too narrow an aperture. It's very important not to insist on thinking about things that seem real today when we could be confronting a dramatic acceleration. So, in the Most Important Century series, I try to just give one concrete imaginable example that won't necessarily be the right one of how technology could get super crazy advanced and give us a huge amount of control over our environment. I call the example digital people. This is very related to the idea of mind uploads. And it's the idea that you could have sort of people made of software, fully conscious beings. They don't have to be just like us, but they would be something that we decided were properly referred to as our descendants, some sort of digital mind that we would look at and say, I care about that digital mind. That digital mind matters. And identify with them. Maybe I think of them as part of our civilization. And just from that, just from having either a digital running of a conscious human mind — and we could talk about whether such a thing would be conscious or what that would even mean, happy to get to that if you want — or just something else that we identify with for any reason, that could get extremely radical because any kind of digital being — it could be in a virtual environment, which means that it experiences anything that the runners of the environment want it to experience — that can be unlimited good. That could be unlimited bad. There's no reason that digital people need to have any kind of health problems or any kind of lifespan limits. And I'm not saying this is necessarily a good thing. There are very scary things about the idea of digital people. You can imagine a world where people are able to lock in certain values to their society forever because of the enormous amount of control you would have over those virtual environments. So I think digital people are something where if you just imagine it's possible, you visualize it to be concrete, you draw out the implications, it's very easy to get very quickly to a utopia or a dystopia that just goes all across the galaxy and lasts forever. Now, is that literally what's going to happen? No, it's an example of what could happen if technology advances enough. And there's probably many more I haven't thought of."
2021-10-05,Ezra Klein,I understand the idea of AI.
2021-10-05,Holden Karnofsky,"Yeah, sure."
2021-10-05,Ezra Klein,Digital people is a slightly different concept.
2021-10-05,Holden Karnofsky,"Yeah, it is."
2021-10-05,Ezra Klein,"What makes you think that that is something that we could do at a level of fidelity that we should be thinking of them as conscious, having rights, people we could simulate on? You have an interesting idea that one way digital people could really help is by accelerating social science. You could see what happens if you make bunches that meditate two hours a day every day, and others don't. But that you could have the fidelity in them that a simulation of a digital people would be relevant to, I guess, we'd call them real people, physical people. Give me the plausibility argument."
2021-10-05,Holden Karnofsky,"I mean, there's many ways you might imagine having digital beings that we cared about for one reason or another, but the plausibility argument I talk about, just the most concrete, is just, what if we could simulate everything that's going on in a brain? And brains are not these enormous rare artifacts. There's a lot of them. And there just doesn't seem to be a particularly good reason to imagine that we wouldn't be able to find some way of simulating a brain so that it is performing all the same important operations, all the same information processing, turning the same inputs into the same outputs, especially if you're imagining a dramatic acceleration in the population of A.I.s, which could be doing the science, which could be understanding how to run these simulations, which could be providing the computers that we've talked about. You could have as many as you want. This is not something where I've decided, oh, yes, this is definitely doable. This is just something that's more like, I don't see why not. I think if we were in this limit of having sort of infinite computing resources, I would expect this to happen by default, but maybe it won't. But it's definitely an example of a way the world could go very wacky very quickly."
2021-10-05,Ezra Klein,"This loops us back to that economic feedback idea because this is something I think is really important in your thought. And it is worth a sitting for a minute, which is that you really identify the core constraint on growth as population. And I don't think that's where people tend to think about it. They think of resources. They think about ideas. But in a lot of these different ideas, whether it's A.I. — and it does not have to be sentient; it can just be idea-generating A.I. that never has a feeling of its own, whether it's digital people, simulations of us, whatever it is, you're imagining a world where you just completely blow the top off of the population constraint. So tell me a bit about the population constraint as it operates in your thinking. Why do you see that as what would unlock so much potential for humanity?"
2021-10-05,Holden Karnofsky,". This is largely coming from the different models that economists are using to theoretically model growth. And they all have this kind of point of commonality. Not all of them, but it's just very common. And Open Philanthropy, we did a report called ""Report on Explosive Growth"" where we kind of very broadly surveyed the economic literature on how people model growth. And there's a lot of ways of doing it. But this feedback loop is just a very common property. It's a very common theme. And it's true that I think most people don't tend to think of population as this essential input into growth. But I think that comes back to long-term, short-term again. A cool paper that I recently wrote about, there's a paper by Chad Jones that just talks about what you think about long-run growth if you're using what he calls the semi endogenous growth models. And it does say that over the last 100 years, a lot of our growth has not been about population. It has not been about population growth. It's been about increasing education. It's been about reallocation of resources. But these things, they have limits to how much more of them we can do. In the long-run, it's kind of hard to see how you get continuing high growth or continuing improvement in living standards per capita if you're not having a rising population. So I think the idea of population is that humans are the only things that produce ideas, innovations, new technologies. You need them to do that. You need more of them if you want more of that. If ideas are getting harder to find over time, then you need even more of them over time. And so, this becomes this very important factor."
2021-10-05,Ezra Klein,"So I do think taking the dystopic possibilities of all this seriously is important. Something the sci-fi writer Ted Chiang once said to me has stuck in my mind when I asked him about if he thought we'd ever create a sentient A.I. And he said, no, he didn't really think so, but maybe we could. But we shouldn't because far before we ever created sentient A.I. that could kill us all, we would create A.I. that we could make suffer dramatically. When I hear about this possibility of creating these virtual environments, whether or not it is digital people or just us people, our people, I've always said that the most likely dystopia in the next couple hundred years always feels to me like the Ready Player One dystopia, where for vast amounts of the population, the actual opportunities in the world have just become so diminished that they are in a Brave New World-like V.R. rig. And things have just become artificial in a thin way as a way of escaping from a grim reality. When you say you're not sure if this is good or bad, I think that the implication when we talk about growth is that it's all going to be great. What are the things that worry you here?"
2021-10-05,Holden Karnofsky,"Things that worry me, number one would be misaligned A.I.s. So I take the possibility seriously. And I know it sounds wacky and like a sci-fi movie, but I take the possibility seriously that things could just spin out of control. You've got A.I.s that can grow the population in a much faster way than humans. And you've got A.I.s just running the world, setting up whatever they're setting up over the galaxy, based on their own sort of random objectives that came in because they weren't carefully designed and are not compatible with humans. So that seems like probably the thing that I feel best about saying, let's really make sure we don't get that. That just doesn't seem like a human-friendly world in any way, shape, or form. Then, what are other things I worry about? I mean, in general, I don't like the idea of rushing headlong into this world, of having a world where all the people who have inclusive and humanitarian values are focused on other issues. Everyone working on A.I. is just someone who loves doing fun, sexy stuff and pushing tech forward. Then we kind of rush headlong into this world where it just becomes a race. And it just becomes — you have this dramatic acceleration. And the people who are the least careful and the most eager to win the race end up running the world. And those are not the kind of people that I hope will end up running the world. So, those are two things I worry about a lot. And a lot of what feels central to me when I think about what do I hope for, if this is the most important century, is, I hope we find a way to have more reflection than either of those implies. There's this term Toby Ord uses in the book, ""The Precipice,"" called the long reflection. And it's the idea of having civilization take its time, in a sense. Use technology to give itself time to improve and to have moral progress and to not build some incredibly large, powerful, galaxy-spanning civilization before it's something that can be done in a fair, inclusive, thoughtful way."
2021-10-05,Ezra Klein,"So, one of the ideas here that you talk about is also that we're in what I've come to think of as a quarter of existential risk. That until the 20th century, humanity hadn't really created technologies that could possibly wipe out all of humanity without a really tremendous amount of effort, right? You really would have needed a lot of people going around, individually killing everybody else. But then we created nuclear weapons, and all of a sudden, we could. We've talked a little bit about the possibility of synthetic bioweapons. Misaligned A.I. is something people worry about on this score. This is a big point of Toby Ord's book. We are one of the first generations to really have the power to end human life. And we're not that wise. Our technology has advanced far quicker than our governing institutions, far quicker than our moral intuitions, I would say. And so, a point you make is that if you get to the other side of this technological era, if you can diversify humanity into other worlds, into other formats of being alive, like computers, maybe there isn't quite as much existential risk anymore. But between here and there, there's a lot. That also strikes me as a reason this is an important period because we may be more protected from wiping out our civilization in a couple hundred years, but we're not now."
2021-10-05,Holden Karnofsky,"Yeah, that all sounds right to me. There is some future state where we've likely become very technologically advanced, where there's not a lot more advances around to kind of shake things up, where there is a great amount of control over the environment, where we're able to be spread throughout the galaxy, have the stability that comes with that, where we might have the stability that comes with not having aging or dying be necessary or just having those take a very long time. So there's a lot of reasons I think that you might expect just the world to reach a state at some later date where however things are, that's how they're going to stay, at least for very long periods of time. And I think that could be a very good thing or a very bad thing. It just depends on what that state is. And that's what gives me so much vertigo about the idea of the most important century. It's just, things speed up a lot. And you get to some sort of radical future. And the seed of it, the starting point, seems like it matters an awful lot. And it's this huge opportunity for things to go very well or very poorly. And I don't feel like there's a lot of seriousness about what to do with that situation."
2021-10-05,Ezra Klein,"So should we maybe just not? Should we maybe just not be trying to build A.I., maybe just not be trying to push some of these technologies forward? Should we maybe just not be trying to grow as much as we try to grow?"
2021-10-05,Holden Karnofsky,"I think if it were up to me, we would be really slow and deliberate and reflective about it and sort of have some process for making sure that a lot of people were included in a serious reflection on whether this is a good idea before anything really dramatic and irreversible happens. It's not up to me. I think this is coming. I think the economic incentives to create this kind of A.I. are very strong. The idea of having the whole world just not work on it doesn't seem viable. I also think there are tremendous potential benefits from A.I. and from all technology. Like I've said, I think that the pattern of economic growth seeming like a good thing does look true over the last couple hundred years and probably is true over the coming decades, at least. So I just don't know that it's realistic to go the way you would kind of go in a happy dream world. And I think instead, we have to take this as something that's coming our way and think about what to do about it."
2021-10-05,Ezra Klein,"Does the entire vision for you actually rely here on this kind of A.I.? Which is to say that if, for some reason, it just turned out that you couldn't create this kind of transformational A.I., that it was never able to learn on enough real world data to create real world innovations, that the computing was too difficult — I don't know — something was found, and it just doesn't — there's a limit we hit and we can't go beyond that on creating computer intelligence, does then the future just look pretty normal to you?"
2021-10-05,Holden Karnofsky,"It looks a lot more normal. It would definitely be a big change in my views. It would remain the case that we live in a very strange time. And I feel like we should be ready for wild stuff to happen next. It would remain the case that anything that kind of went into that feedback loop or dramatically accelerated science and technology for any other reason could be enormously high stakes. There are other technologies I could name that I would consider to be still extremely important, still extremely high stakes. I think the climate change situation would be one of the biggest things looming in sort of this could make a permanent difference kind of zone. So would the risk of pandemics. So I think there'd still be a lot to think about. I think we would still live in an extraordinarily strange time. But learning that there's no kind of digital mind that could do what humans do to advance science and technology, including a simulation of humans, a digital simulation of humans learning — that's just off the table — that would dramatically move me toward expecting stagnation and thinking about what to do about that."
2021-10-05,Ezra Klein,"So let's live in that world for a minute. Let's not think about digital people, A.I. minds, none of that. Something you tapped earlier is the possibility of easing the resource constraint, because you are mining the incredibly abundant minerals and metals and asteroids and in other parts of the galaxy. Even before you get to settling all over the galaxy, you actually could get a lot of resources out of things that are just flying around space. How important is that as a set of technologies for opening the resource constraint, as opposed to the population one?"
2021-10-05,Holden Karnofsky,"That's not something I have really strong views about. I think there's clearly resources that are important for us that we're not going to be able to just get that way. Having a habitable planet and a friendly climate to humans, you can't just take a bunch of metal and somehow make a friendly climate out of that. So I do think eventually, it should be possible to get all our energy from clean sources. But I think there is a real possibility that if this A.I. thing turns out not to be feasible, we could be looking at a world where we really do stagnate in terms of living standards, and we don't have as much of a growing positive-sum economy anymore. And that becomes a different kind of challenge."
2021-10-05,Ezra Klein,Would that be a bad thing?
2021-10-05,Holden Karnofsky,"I would think by default, yes. It seems like you've got kind of a wind at your back if there's more to go around every year, more wealth, more just things that make people's lives better. And there is still an enormous amount of poverty today that is just really severe and is quite material. So it seems like there's still quite a lot of work to do in reducing poverty. It's a lot of what Open Philanthropy focuses on. It's what I've spent most of my career on. And I think it's really important. So, yeah, I think the idea that you stagnate and the idea that you just have what you have, I hope it would happen after there's no more poverty of the kind that we see in lots of the world today. But even if that happens, I think it makes me nervous to have a world where there's no wind at your back of that kind. And it just, we have what we have, and we're all just fighting about who gets the most of it."
2021-10-05,Ezra Klein,"Well, let me take the other side of the argument for it to be provocative."
2021-10-05,Holden Karnofsky,"Yeah, sure."
2021-10-05,Ezra Klein,"So when I talk to you or anyone else who believes in something like the most important century, I hear a lot of, I'm not saying this is good or bad. I hear a lot of, we could invent misaligned A.I. that wipes out all of humanity. I hear a lot of, maybe we'll create digital people or at least some kind of morally important, morally consequential digital life form. And I look around, and I look at how we treat animals. And I think, well, that may not go well at all."
2021-10-05,Holden Karnofsky,Agree.
2021-10-05,Ezra Klein,"I look at the creation, of course, of weaponry, which you've not talked that much about here, but that could be unbelievably destructive. And there's, of course, an alternative vision, which is, I had Richard Powers, the novelist, on the show a little bit back, who wrote Overstory, just wrote the beautiful book ""Bewilderment."" And I mean, his view of where humanity should go is not non-technological, but it is into more of a relationship with nature. I think we called it, in a nutshell, a scientific animism, that the world is getting richer, but we don't need to be that much richer for there to be a lot to go around. We have the capacity to, over the next 30 or 40 or 50 years, power things through solar if we chose to. I mean, the same constraints on our wisdom that keep us from responding to climate change well now, they bite as hard or harder in the kinds of futures you're talking about. So, maybe the future we should be working towards is not this unbelievably high risk gamble of spreading out throughout the stars and maybe enslaving A.I. or being enslaved by it, but just trying to chill a little bit, share a bit more, protect the climate. Why should that not be the vision?"
2021-10-05,Holden Karnofsky,"One, my answer is it doesn't seem to be an option. I mean, we try to focus on things we can do today that will be helpful for the world we're actually heading toward. And if this kind of A.I. being a giant driver of acceleration happens to come out, I mean, I don't think there's much I can do, sitting here today, to just decide that's not the way we're going to go. In the other world, I mean, I think that vision kind of sounds nice at a high level, but there's just an awful lot of horrible grinding poverty in the world today. And so I don't think that would be a very appealing vision until people have a basic standard of health care and a standard of living and good nutrition, which a lot of people in the world just still don't have enough calories, are still kind of stunted because they didn't get enough to eat as kids. So I think that's very important. There's a lot of work to do there. And that's not something that's going to happen if we don't see more economic growth. I also think there is — I don't know, but I think the idea of zero economic growth just makes me nervous. I think it should make anyone nervous. Because as long as we have economic growth, there's at least things that are happening in the world that are positive sum. There are projects you can embark on, where you can say, I'm doing this to make everyone's lives better. And at the point where the only change in resources is zero sum, the only thing that helps one person hurts another person, that's a different dynamic than we've had over the last few hundred years, during which, as I said, it looks like life has been improving. So it's at least something that makes me nervous. It's just, I don't know what the world looks like in that kind of situation. Maybe it turns out wonderful. Maybe we all learn to just have perfect equanimity and live together in harmony and appreciate what we have. But it does make me nervous."
2021-10-05,Ezra Klein,"But I want to pick on this for another minute because I do think it gets to a constraint people will take seriously in a low growth world and then don't take that seriously in the hypergrowth world — say they do, but don't — which is you were just saying that there are many, many people who don't get enough calories around the world. We could change that."
2021-10-05,Holden Karnofsky,Sure.
2021-10-05,Ezra Klein,We could.
2021-10-05,Holden Karnofsky,Agree.
2021-10-05,Ezra Klein,"We could change our energy systems. Many of the things that make the world we live in at its current state way worse for billions of people than it needs to be are solvable, and we don't solve them. I mean, not only do we not solve them, we are not going to get close to vaccinating lower income countries this year. We could. This needed a couple billion bucks and a good global mobilization. And we're not going to do it, even though we could absolutely afford to do it. And the same shortsightedness, the same lack of compassion, the same inability — actually, I'd say, an expanded inability— as hard as it might be to see somebody in another country as kin, it will be that much harder to see a digital something rather as kin. Hard as it is to see a cow as kin, again, I think the way we will treat non-player characters in this world could be really scary. And so, if we haven't been able to make something closer to a paradise out of this world, I'm not saying why should I believe we will make the hypergrowth world better. I'm more saying that why should I want to give us a chance to make it as bad as we could."
2021-10-05,Holden Karnofsky,"I mean, first off, I would say that we haven't solved the problems that exist today, but we've made a lot of progress on them. And this is where I agree with the news floating around about how the last few hundred years look for humanity. They look good. It looks like there's been a lot of progress on a lot of things. So, there has been progress. And we do get somewhere. I mean, we don't get immediately to where I wish we were as fast as I think we could. But I'm not sure exactly what you're asking here. Are you asking about why the hyper —"
2021-10-05,Ezra Klein,"I'm making you — well, I'm trying to just make you question some assumptions."
2021-10-05,Holden Karnofsky,Which assumptions do you want me to question?
2021-10-05,Ezra Klein,"Simply the assumption that if you want to say you don't know if all this will be good or bad, maybe the possibility that it will be bad makes it such that we should be stopping people from trying to do it. And that if we can even get the world we have right, a world with this kind of dizzying change may be beyond human capacity to manage in any kind of even basically decent way. And so we should fear it as more likely a dystopia than a utopia or even something in the middle."
2021-10-05,Holden Karnofsky,"Yeah, I basically am just in agreement as a maybe — I mean, as this is a possibility. I mean, I'm just in agreement with you. I'm not sure I have an assumption that contradicts anything you said. I think where I am sitting is I'm saying, do I want my role to be trying to stop people from building powerful technologies to address real problems that exist in the world today because of this risk it might accelerate? I mean, I don't want my role to be that, partly because it would be a strange thing to be doing, based on some grand vision for how the long-run future is going to go that I don't think I have. And I do think we have short-term problems that are worth addressing. But also, I don't know — I don't think that's a path that looks very promising to try and go down. And the three criteria for Open Philanthropy's work are importance, neglectedness and tractability. So importance is how big a deal is an issue, how many people does it affect. Neglectedness is how much attention is it getting. We'd rather work on issues that get less attention. And tractability is what can we do about it. Do we see a path to victory? Are we going to matter? Open Philanthropy cares about all three of those. So when I think about the possibility of the most important century, I'm looking for paths and things we can do that will make the best of it."
2021-10-05,Ezra Klein,"So let's say you have somebody listening, and you buy into this, or at least, you buy into it as a real possibility. And you want to try to help make the best of it. What is tractable for an individual here? What does this imply for just somebody's life, if they want to try to live on this time scale?"
2021-10-05,Holden Karnofsky,"The first answer I want to give is that I wish I had a better answer. I wish I could just tell you, hey, I've not only figured out what's going to happen, which I haven't — I figured out what needs more attention. But I figured out exactly what to do about it. But I haven't done that either. And one of the things I say in my blog post series is, if you're thinking, hey, this could be a billion-dollar company, maybe the right reaction is, yeah, awesome, let's go for it. And if this could be the most important century, I think my reaction tends to be just like, ooh. I don't know what to do with this. I need to sit down. I need to think about it. I think there's a number of reasons that it's actually very hard to know what to do. And I think we need more attention and more thought and more reflection because most things that I could name as an attempt to make the most important century better could be really good or really bad, depending on your model of what the most important considerations are. So I can get to that in a second, but I don't want to be totally gloomy about it. I think there are things that look robustly helpful. There are things that look good. One of them is A.I. alignment research. So just, if we could get to a point where we're capable of being confident that we can build advanced A.I. systems that aren't just going to run the world according to whatever objectives they have, and that is a field that exists. That's a field that people are working on. We fund work in it. Another robustly helpful thing is just trying to find and empower more people who are seeing the situation as it is, taking it seriously, being as thoughtful about it as they can be, and approaching it from the perspective of what's best for humanity, instead of what's best for them narrowly. So I do think there are some activities. And there's some discussion of it in the blog post series."
2021-10-05,Ezra Klein,"And then finally, always our last question, what are three books that have influenced you you'd recommend to the audience?"
2021-10-05,Holden Karnofsky,"First book is called ""Due Diligence"" by David Roodman. He's my coworker now, although he wasn't at the time he wrote the book. It's about microfinance, which is very small loans to low-income people. And there was this massive debate about how much microfinance helps people, whether it's the best way to help people. And ""Due Diligence"" is an attempt to sort through a very overwhelming debate among experts and academics from looking at it from multiple angles, getting the right answer. I think it's a great book to read about how to think. I think it's better for that than a lot of books about how to think, although I generally think writing is a better way to learn critical thinking than reading anything. I think this is one of the best things to read for that. Book two is called ""The Lifeways of Hunter-Gatherers"" by Robert Kelly. I want to be upfront. It has a lot of really dry theoretical parts. There's many parts I skipped entirely. There's many parts that I've read maybe a dozen times by now. It's trying to pull together all the evidence we have about what life is like for people who live in a hunter-gatherer lifestyle, which is often assumed to represent what the distant past of humanity looks like, before the Neolithic Revolution 10,000 years or so ago. And it's interesting to see how someone tries to piece together the clues about our very distant past. And like I've said, I've generally been obsessed about thinking about long time frames and getting my head into that space. Third book is about the future. I'll recommend ""The Precipice"" by Toby Ord, which we've talked about. It's about the idea of existential risk. It goes through the different things that might happen that could permanently cut off humanity's future. I think it's a really good read. And then I want to throw in two bonus books that haven't been published yet."
2021-10-05,Ezra Klein,Do it.
2021-10-05,Holden Karnofsky,"So one of them is ""What We Owe the Future"" by the Scottish philosopher Will MacAskill. It comes out, I think, in September 2022. And it's about the idea of long-termism. It's about what are the things we could do today that could matter to all the future generations. And shouldn't we care about that, when those future generations don't have a voice in the things we're doing today that affect them?And the final book does not have a title yet. I can't promise it will ever exist. But it's by my wife, Daniela Amodei. And I would expect it in a few years. And it's about the history of people using science and technology to get better gender equality, so things like the pill, things like formula feeding, so that the whole burden of childcare doesn't fall on the woman, things like I.V.F. for more options in reproductive timing, and about how in the future, it's going to be necessary to use more science and technology and to have better science and technology to achieve better gender equality."
2021-10-05,Ezra Klein,"Holden Karnofsky, your blog is Cold Takes, which we will put a link to in the show notes. Thank you very much."
2021-10-05,Holden Karnofsky,Thank you.
2021-06-11,Ezra Klein,"So a few months ago, I came across this really fascinating essay by Sam Altman called ""Moore's Law for Everything."" Altman is the C.E.O. of OpenAI, which is one of the biggest and most interesting of the companies trying to create general purpose artificial intelligence. So an A.I. that can improve itself and learn and actually do all sorts of things in the economy and in the world. I've met him a few times. And he is a believer. He sees a world coming. He believes he is bringing a world into existence that is really, really different than the one you and I know and that it's going to happen fast. And who knows, maybe he's right. But what caught my eye about this essay, ""Moore's Law for Everything,"" is Altman's effort to try and imagine the political consequences of true artificial intelligence and the policies that could decide whether it ushers in utopia or dystopia. So Moore's law is the observation that the number of transistors on microchips doubles every two years. Or to put it more simply, that computer power has been growing exponentially for decades now. But prices have actually been falling. That's not been the case for housing, or health care, or higher education. But what if it was? Altman's argument is that true A.I. could get us closer to Moore's law for everything. It can make everything better, even as it makes everything cheaper. And I have some points of skepticism here. You're going to hear them in the conversation. But maybe he's right. I hope he's right. That would be good. But if you grant his argument, if he is right, then he says that the world looks like this — A.I. will create phenomenal wealth, but it will do so by driving the price of a lot of labor to basically zero. That is how everything gets cheaper. It's also how a lot of people lose their jobs. To make that world a good world for people, to make that a utopia rather than a dystopia, it requires really radical policy change to make sure the wealth A.I. creates is distributed broadly. But if we can do that, he says, well, then we can improve the standard of living for people more than we ever have before in less time. So Altman's got some proposals here for how we can do that. They're largely proposals to tax wealth and land. And I push on them here. But more than that, I want to push on and interrogate the political economy behind them. Because if he's right — and I assign a real possibility that he is right, at least to some degree — then A.I., it's not just going to redistribute wealth and jobs, it's going to redistribute power. And these people who will then have so much more power, they're going to offer themselves up for more taxation? They're going to give up parts of their company every year and their wealth and do it willingly? I'm not sure. So I want to offer this warning before we get into the show. If you don't spend a lot of time in the A.I. debates, you're going to hear some stuff in this conversation that just sounds weird to you. And I think it's important not to get scared off by that. Go back over the past few hundred years. There are times when technology just upends society. We may or may not be on the cusp of one of those times now. But on the chance that we are, even if it's only 50% as much as Altman thinks, then it is important to think about the politics of that world, to try to be ahead of it not just catching up to it. And that's what this conversation is about. As always, my email is ezrakleinshow@nytimes. com. Sam Altman, welcome to the show."
2021-06-11,Sam Altman,Thank you for having me.
2021-06-11,Ezra Klein,So let's start where we are now. What can A.I. do now?
2021-06-11,Sam Altman,"So I think we have just begun the realm of A.I. being able to be what we call general purpose A.I. instead of these narrow models that can do one thing. So now we have models that can understand to some sense, to some definition of that word, the world, what's going on, and then a single model can accomplish a wide variety of tasks for someone and can learn new things pretty quickly. So last year, OpenAI released something called GPT-3. It's a large text model. And that one model is now being used by thousands of people — developers more than that in terms of end users — for all sorts of tasks. And I think we will see, in the coming years, A.I. medical advisors that give anyone really high quality medical advice, higher than they can maybe get anywhere in the world, A.I. tutors that can teach you math or other topics. We're starting to see people develop A.I. programming assistance that can help you write code. And these same models are being adapted for all of these different uses. And I think we're starting now heading into a world where a lot of the things that people want, they can, through a text interface or a visual interface, have an A.I. help them do."
2021-06-11,Ezra Klein,"As I understand what these systems are doing now, they're predictive. How is that intelligence?"
2021-06-11,Sam Altman,"So let's think about what you need to make a smart agent. You need to understand what's going on in the world, and you need to be able to think of new things, and you need to be able to take action to accomplish goals. In a very deep sense, I think the biggest miracle that we need to create the super powerful A.I. is already behind us. It's already in the rearview mirror. We just — we need an algorithm that could learn and a network architecture that could somehow encode the knowledge and the capability to reason. And then once we have that, however imperfect it is, if you apply it to bigger and bigger systems with more and more computing resources, there's no obvious upper limit. Once you have a system that can take in observations about the world, learn to make sense of them — and one way to do that is to predict what's going to happen next — I think that is very near intelligence. So we have the ability to build systems that can learn, update, remember, create. We have the ability to build agents that try to accomplish a goal with that knowledge. And we've gotten surprisingly far just putting those few simple ideas together."
2021-06-11,Ezra Klein,"And so if I basically understand how GPT-3 works, it's a system that has read a lot of stuff on the internet."
2021-06-11,Sam Altman,Yes.
2021-06-11,Ezra Klein,And it's predicting the next word in the sequence.
2021-06-11,Sam Altman,"Slightly oversimplified but very close. Yes, it is trying to predict what comes next in a sequence."
2021-06-11,Ezra Klein,"And so to what you're saying then, we have figured out algorithms that can learn."
2021-06-11,Sam Altman,Yep.
2021-06-11,Ezra Klein,"And then as they learn and as they know more, then as we are able to ask them to orient their predictions in different directions — that's basically —"
2021-06-11,Sam Altman,Yeah.
2021-06-11,Ezra Klein,— a generalized intelligence that we could —
2021-06-11,Sam Altman,And there's —
2021-06-11,Ezra Klein,— that we could use.
2021-06-11,Sam Altman,"There's no obvious upper bound to that process. Think about how a kid works. They observe the world. They start to learn. They clearly make some predictions. And you can see when something doesn't go the way they think it's going to go and then they update. And the next time they're not surprised or maybe they even can come up with new ideas based off of that. And you can also teach them stuff. You can correct them. And this idea of giving human feedback to a neural network in the way that we've given to our children for a long time, where we say, no, that's right, no, that's wrong, and the model, the A.I., or the kid, or whatever, is updated each time that happens. But then, let's say, the agent can make better predictions, and do better, and need to get corrected less and less on increasingly subtle issues. There is no upper bound how far that can go as you think about increasing the size and scale."
2021-06-11,Ezra Klein,"I, obviously, don't know how to use GPT-3 the way you do or what's coming after it. But it sometimes seems to me that the harder thing to see from the outside of where all these are going is how flexible they are or they will become in being able to understand the way I want them to make a prediction. So there's a lot that is being predicted, but right now it's a mimicry. Like do stuff that is like the other thing."
2021-06-11,Sam Altman,Have you tried our newest instruction following model by chance?
2021-06-11,Ezra Klein,I'm not sure.
2021-06-11,Sam Altman,"You should. I think we've released something like that just quite recently where you can say, here's what I want you to do and it will do it."
2021-06-11,Ezra Klein,What have you done with it? What can it do?
2021-06-11,Sam Altman,"A lot of — it's still very simple, but a lot of simple tasks. If you say, summarize this for me or translate this into French, no matter what language it's coming in as, or even a small multi-step problem where you need it to manipulate natural language, you can just say what you want it to do. And you can even say — I mean, you could probably say, write this in the style of Ezra Klein, and it would probably just do it."
2021-06-11,Ezra Klein,What is the way A.I. changes the economy or changes people's lives without using the word A.I.? What is the force it exerts on economic activity?
2021-06-11,Sam Altman,"It's a great question, Ezra. There's a lot of different ways to answer that. But this idea that things that humans used to have to do that maybe they didn't want to do, that took a lot of time, that took a lot of effort, that weren't done that well, automated systems that can do those better. So we saw this happen with translation, for example, where if you couldn't afford a translator, which most people couldn't, and you were going to wander around some foreign country — I remember what that was like before we had A.I. translation and I know what it's like now where I just speak into my phone and it speaks out the other language. And I love that. That really changes my experience of traveling. We're beginning to see that now with certain subsets of the medical field where A.I.-assisted doctors can do better than doctors on their own, or let's say computer-assisted doctors. But this idea that you can have intelligence, smartness, creativity — whatever you want to call it — that a computer is doing to help you do the things you want and have the life you want at a marginal cost of zero or very close to it, that's I think a transformative thing happening in the world."
2021-06-11,Ezra Klein,"So that last piece is a piece I want to focus on, that marginal cost of zero. So the idea is that as machine learning, as these computer systems become better, basically everyone will have at their disposal a staff, like a corporation unto themselves, where they will be able to hire systems to help them out —"
2021-06-11,Sam Altman,Yes.
2021-06-11,Ezra Klein,— for almost nothing.
2021-06-11,Sam Altman,Yes.
2021-06-11,Ezra Klein,"Of course, that means — you were framing this a couple of minutes ago in terms of jobs people maybe don't want to do or don't do that well. But that also means jobs people do want to do or did do reasonably well, but not as well, go away."
2021-06-11,Sam Altman,"That's been happening for a long time and I don't think we do a service to anyone to pretend that technology does not eliminate some jobs. Some jobs it just makes much better. But the arc of this has been that every technological revolution eliminates one class, one sets of jobs, and we find new ones on the other side that are hard to imagine from where we sit today. So before we had the computer, it would have been hard to imagine the computer programmer would be such an important profession. But I think if you look back at the great technological revolutions, which have been the punctuations where there's been a lot of shift at once, there's always been this worry. We've always found other jobs on the other side. Now, it may be that this time it's different, right? If we really do think about what it means to have intelligence in a computer, maybe it's different. But I so deeply believe that human ingenuity and desire for ever sillier kinds of status is so unlimited that we will find a lot of new things to do. I also think what we are likely to find is that a lot of classes of jobs that people talk about A.I. taking away stay, but the role of the human is very different. And the human does what humans are really good at. The A.I. does the part that the human might like to do less and that the A.I. is really, really good at. And you see this synergistic effect — I hate that word, but I couldn't think of something else — where humans continue to be programmers, or doctors, or whatever."
2021-06-11,Ezra Klein,Journalists maybe.
2021-06-11,Sam Altman,Journalists. But they focus on a different part of the job.
2021-06-11,Ezra Klein,"Two places I want to go with that. One is I'm usually on the side of this argument that says I don't think machine learning is going to bring a near-term job apocalypse. But I've become friendlier in the past couple of years to the idea that it will destroy a lot of jobs pretty quickly. And to me it gets to the thing you were saying about generalized systems. In a lot of past revolutions, the revolutions, the technological changes, they happened fairly slowly and particularly their dispersal through society is pretty slow. And then second, they are general over time but they're not general all at once. Now, if you think of machine learning as part of — and particularly of what we're talking about here — as part of a long swing of automation, it isn't all at once. But as I understand the way you see the world, that you think this is going to happen in a much more punctuated period than, say, electricity."
2021-06-11,Sam Altman,"A couple of years ago, if you talked about general purpose A.I. at all, people said that's ridiculous, it's not happening. If you talked about systems that could really do meta-learning and learn new concepts quickly that they weren't trained for, people so that's not going to happen. And we've gone from a world where many of the experts in the field said that was sci-fi and irresponsible to talk about to clear existential proof that we have it. And it certainly doesn't seem to be slowing down. Moore's law, in varying definitions — but let's say that was like a doubling of transistors every two years — maybe A.I. is growing at a rate of 10X per year in terms of these model sizes and the associated capabilities. So I do think we're on a very steep curve. We will hit limits, but we don't know where those will be. We'll also discover new things that are really powerful. We don't know what those will be either. We're deep in the scientific discovery phase, which is awesome. It's so exciting and fun. But I think what we can say is that we are on an exponential curve. And when you're on an exponential curve, you should generally, in my opinion, take the assumption that it's going to keep going. And humans are very bad at intuition for this. Trying to think about exponential curves for stock prices, for technology, for population growth, whatever —"
2021-06-11,Ezra Klein,For viruses.
2021-06-11,Sam Altman,"Viruses. Very difficult. I had this moment that I'm sure you had one, too, when it looked like COVID was really going to take off and most of the world wasn't paying attention. And I was walking the streets of San Francisco one night at like 10:00 p. m. walking home and everybody was just like frolicking and doing their thing. And it was just like, this is the last moment of normalcy and no one's paying attention, because most people don't understand exponential curves. And it was such a strange feeling and I've often thought about the parallels of that moment that hit so viscerally with A.I."
2021-06-11,Ezra Klein,"My own version of that — I remember sending an email to my entire family and saying, you need to go to the grocery store. In three weeks, you're going to be living in a different world and you need to listen to me on this. Let's talk about where exponential growth gets us in. You're saying there's a 10X-ing every year. That's pretty big."
2021-06-11,Sam Altman,Yeah.
2021-06-11,Ezra Klein,"20 years from now, what are you confident A.I. will be able to do and what do you think it is likely that it will be able to do?"
2021-06-11,Sam Altman,"Man, 20 years was a hard time frame."
2021-06-11,Ezra Klein,Let's do 10.
2021-06-11,Sam Altman,"Let's do 10. 10 I can do. In 10 years, I think we will have basically chat bots that work for an expert in any domain you'd like. So you will be able to ask an expert doctor, an expert teacher, an expert lawyer whatever you need and have those systems go accomplish things for you. So you're like, I need a contract that says this. I need a diagnosis for this problem. I need you to go book me this flight. I want a movie created. I want you to make me an animated short or a photo realistic short that looks like this. I need you to help me write this computer program. So let's say most repetitive human work and some creative human work you will be able to ask an A.I. to do for you. And that is a massively transformative thing."
2021-06-11,Ezra Klein,"I want to get at whether we are talking about a singular when we talk about an A.I. system or a plural. So the natural metaphor in people's mind will be a software program. I buy some stuff from Microsoft. I buy some stuff from Apple. I buy some stuff from all kinds of developers whose names I forgot. My understanding of the way the A.I. systems are developing is that it's turned out using the big machine learning neural networks that have turned out to be successful. You need ungodly amounts of data and amounts of computer power. And that means only a couple groups are big enough to build these general intelligence systems, assuming they do get built. And so is the world you're talking about here a world in which OpenAI and maybe a couple other players have a system that is licensed out and everybody is building on top of that? Is it all just coming from you? What is happening here economically?"
2021-06-11,Sam Altman,"I think it's going to be somewhere in between these narrow models that anybody creates and these mega models that only a few people can create. There's this concept of fine-tuning. So where someone like OpenAI creates this powerful base model that only a few organizations in the world can do, but then maybe want to use that for a chatbot, or a customer service agent, or a creative multiplayer video game. And you take that base model and then with just a little bit of extra training and data, you push it in one direction or the other. So I could easily imagine a world where a few people generate these base models and then there's the medical version, the legal version, whatever else, that get fine-tuned or polished in one direction or another. And a lot of people — a lot more people are capable of doing that. We're starting to experiment with offering that to our customers now. But I could see a world — and I think it does no one any good to pretend otherwise — where as these models get really smart, the general purpose one can just do everything really well. And this idea that we think right now — that OpenAI thinks — which is we're going to push one to be a coding expert and one to be a medical expert, turns out not to be necessary because 10X compounding is just so powerful that 10 to the 10th 10 years from now — the base model is plenty good at everything."
2021-06-11,Ezra Klein,"Staying on the idea of the big base models, as I understand it, there are a little handful of companies who are in the race to build one of these in a serious way. You're one of them. But DeepMind is another. There's a lot of work happening in China. Players in America like Amazon and Apple are trying to create — maybe not always general but pretty big things. How should I feel about the idea these will be privately owned if they're going to be this powerful?"
2021-06-11,Sam Altman,"Strange. If we think about the major big iron engineering projects of the past, like the ones where there were large geopolitical and certainly social consequences — just to pick two examples to talk about here, we could talk about the Apollo project and the Manhattan Project — you can find different cost estimates of what those were. And who knows what you want to believe, but let's say something with the word $100 billion in it. Those really took the wealth of nations to do. No company like us was going to do that, right?But the cost of technology does its thing. Companies and people get wealthier over time. You now do have a world where certainly the mega caps like Google can join this effort. But even much smaller organizations like OpenAI can get enough capital together — barely — to be able to be competitive here with what anybody else can do. So that's weird. I have misgivings about that. Probably the non-technical thing I think most about is — let's say we do make the true AGI, like the one from the sci-fi movies."
2021-06-11,Ezra Klein,The Artificial General Intelligence?
2021-06-11,Sam Altman,"Yeah. How do we want to think about how decisions are made there, how it's governed, who gets to use it, what for, how the wealth that it creates is shared? If this is going to be one of these species-defining moments, it for sure should not be in the hands of a company, certainly not ours. But saying what we want the structure to be there, how we want to make decisions about it, what the equivalent of our Constitution should be, that's new ground for us and we're trying to figure it out now."
2021-06-11,Ezra Klein,OpenAI begins as a nonprofit.
2021-06-11,Sam Altman,Yeah.
2021-06-11,Ezra Klein,"It becomes a for-profit, in part, because it needs to raise money and resources. So it got a billion investment that's partially money, partially compute power from Microsoft."
2021-06-11,Sam Altman,"Actually it was all in cash, but we spent most of it on compute —"
2021-06-11,Ezra Klein,"Oh, there you go."
2021-06-11,Sam Altman,Close enough.
2021-06-11,Ezra Klein,"But partially on Microsoft computing power, correct?"
2021-06-11,Sam Altman,Yeah.
2021-06-11,Ezra Klein,"Yeah. One of the worries I have about this is that even if people want to be very cautious about what the incentives of it are, that just in order to do it, you have to submit to those incentives. That just in order to raise the money, there has to be a business model, a backer. And I was reading that and I wondered this from a different direction, too. Was that a missed opportunity for the public sector? Should it be that the public sector is spending the money to build this either by funding groups like yours or a consortium of academic groups or something?"
2021-06-11,Sam Altman,"A little known fact, we tried to get the public sector to fund us before we went to the capped profit model. There was no interest. But yeah, I think if the country were working a different way — I would say a better way — this would be a public sector project. But it's not and here we are. And I think it's important that there is an effort like ours doing this. That even if not an official American flag effort, we'll represent some of the values that we all hold dear. That's better than a lot of other ways I could imagine someone else doing this project with us going. And one of the incentives that we were very nervous about was the incentive for unlimited profit, where more is always better. And I think you can see ways that's gone wrong with profit, or attention, or usage, or whatever, where if you have this well-meaning people in a room, but they're trying to make a metric go up into the right, some weird stuff can happen. And I think with these very powerful general purpose A.I. systems, in particular, you do not want an incentive to maximize profit indefinitely. So by putting this voluntary cap on ourselves above which none of the employees or investors get any more money — which I think if you do have a powerful A.I., will be somewhat trivial to hit — I think we avoid the worst of the incentives or at least the one that we were most worried about."
2021-06-11,Ezra Klein,"How about speed? So there's an incentive to get there first. There's going to be huge financial returns and otherwise returns to being the first one. You're to some degree in a race with other companies to do it. I'm not saying that leads to cutting corners, but it leads to things where maybe you'd ideally want to wait for a governance structure to emerge, want to wait for a public conversation to happen. Well, if you don't do it, the other folks will. And one of the constant ways things get justified in both government and business is that better us than them."
2021-06-11,Sam Altman,"For sure. So I think we were able to design a system that addressed a lot of the incentives that I was particularly concerned about. The one that remains that I am — for the entire field, not just us — most concerned about is actually closer to the super powerful systems like the ones that people talk about creating an existential risk to humanity where there's a race condition. And that I think will be on us and the other players in the field to put together a sufficient coalition to stop ourselves from racing when safety is in the balance. And we're trying to figure out how to do that. That's part of the governance question. Before you push go on this extremely powerful system, you would like as much time as you can get — and it won't be totally in your control, because some other government can be doing whatever. But you'd like as much time as you can have to be really thoughtful about do we understand what the system is going to do."
2021-06-11,Ezra Klein,And how do you get that time?
2021-06-11,Sam Altman,"You have people partner and say, OK, lots of other industries have done this. I think the recombinant DNA conversations in the '70s are a good example. But you say, we're the experts on this. We're the set of companies with the resources to do this. How can we work together to make sure we all have the time we need?"
2021-06-11,Ezra Klein,"Is there a different kind of pressure that comes from the geopolitical push? It was interesting to me you used the metaphor of an American flag operation. So there's a competition between you and other American companies. And then there is the sense that there's also a competition from China, potentially, certainly down the road other countries. That can create a different kind of pressure. It could even be a pressure coming from the public sector. But a pressure to finish first."
2021-06-11,Sam Altman,"This is something that some parts of the field have spoken about for a long time, which is, sure, the private sector companies can do whatever they want. But what if there's huge public sector pressure? And that I don't think we'd be in a position to have too much of an impact on. You"
2021-06-11,Ezra Klein,"Mentioned before the cap on profits, which I believe for you all is 100X."
2021-06-11,Sam Altman,It started as that. It's come down every time we've raised more money. So it's the —
2021-06-11,Ezra Klein,Got it.
2021-06-11,Sam Altman,— single digits now.
2021-06-11,Ezra Klein,So there's a question of how much money people can make and there's a question of how money gets made.
2021-06-11,Sam Altman,Yeah.
2021-06-11,Ezra Klein,"And I've been thinking a bit about that as an important incentive question. So I'll use your competitors over at Google as an example here. I worry a bit that if the big A.I. system is built by Google, it's going to be oriented towards being very, very, very good at manipulating consumer preferences. It's an advertising-based business. Other great interesting internet companies ended up building themselves around manipulating how people spend money online. I wouldn't like to see something this powerful done that way. But Microsoft has other ones. You guys have other ones. It got me thinking about whether or not there are ways to shape the direction these systems are tuned in in useful ways. Like for instance, you could imagine a prize system where the public sector puts out multibillion dollar prizes for solving this technological problem, this scientific problem."
2021-06-11,Sam Altman,Yeah.
2021-06-11,Ezra Klein,Tell me a bit about this end-game business models here. What they are turning out to be and what the options for them are?
2021-06-11,Sam Altman,"So I can't speak to what Google is going to do other than I probably won't like it. s But I could tell you how we're thinking about it and how I hope other people will, too. What we're excited about doing is the best research in the world and trying to build this eventually quite powerful general purpose system. What I think we're not the best in the world at, nor do we want to really divert our attention to, are all of the wonderful products that will be built on top of this. And so we think about our role as to figure out how to build the most capable A.I. systems in the world and then make them available to anybody who follows our rules to build all of these systems on top of them. We have thought about prizes. You can imagine saying whoever — here's this API. Do something great for the climate and here's an incentive to do it. Or any number of other categories. But I do trust that on the whole if we can deliver super powerful new tools at an affordable price, the market will do its thing and we will get wonderful solutions to some of the most pressing problems facing humanity."
2021-06-11,Ezra Klein,"So you don't think pro-social goals need to be built in by something explicit? You think that — you think certainly once you can just ask questions, at least those questions will be asked and they'll be answerable?"
2021-06-11,Sam Altman,"Yeah, I mean, I think we're going to do it, too, because it seems like a good thing to do. But I do think that even without that incentive, a lot of great pro-social things will happen."
2021-06-11,Ezra Klein,So you have a version of this. You say that the rallying cry for this generation should be a Moore's law for everything.
2021-06-11,Sam Altman,Yeah.
2021-06-11,Ezra Klein,Tell me about that idea.
2021-06-11,Sam Altman,"Look, it's not a popular moment to say something positive about capitalism, but I will anyway. Which is that I think the power of capitalism has done more to help more people than probably any other single idea in human history. And the innovations, the new technology that we have both developed and gotten into the hands of people and the almost unimaginable improvements to quality of life, even though it's fun to talk about how awful the world is and it is in many ways. That's great. And part of the reason that has happened is things have gotten much cheaper. Pick your example. The cost of an hour of light at night, the cost of computing power, the cost of — so many things are now free that when we were kids were not. Entertainment, high quality education. You can go learn anything you want on the internet so well. It's amazing to me."
2021-06-11,Ezra Klein,"My family, when I grew up, a lot of it was in Brazil. And long distance calls were a big deal."
2021-06-11,Sam Altman,Yeah.
2021-06-11,Ezra Klein,And now it's like every morning my kid is on FaceTime with his grandparents.
2021-06-11,Sam Altman,"Yeah. And this relentless march of technology making the things that we need and want cheaper and cheaper and cheaper, that's great. That's how people's lives get better every year. And all of us want our lives — or almost all of us — want our lives to get better every year. I think one of the big promises of A.I. is because of this idea of the marginal cost of labor going towards zero, things can get dramatically cheaper quickly. And way more of us can afford way more of the things that we want. And for a generation that has had a lot of, I think, economic headwinds and a deck stacked against them, I think this is a specific thing we can imagine that gets a lot of people's lives way better pretty quickly."
2021-06-11,Ezra Klein,"So I want to make this specific. When you talk about a Moore's law for everything, things that are expensive, that millennials, zoomers are having trouble affording when it was sometimes easier — sometimes also not — for their parents."
2021-06-11,Sam Altman,For sure.
2021-06-11,Ezra Klein,"But housing is very, very big in this. Health care is big in this."
2021-06-11,Sam Altman,"Higher education, I think if we look at those three."
2021-06-11,Ezra Klein,"Higher education is huge. Those three are big. And I would say housing is primarily a zoning problem. In health, technology and the cost of technology is a real player there. And then there's a fair amount of regulatory questions in both of them. So how does building A.I. allow for a Moore's law in health education and housing?"
2021-06-11,Sam Altman,"So I think technology is a necessary, but not sufficient, part of the solutions here. I think that if we can't get the politics and the policy right, it will face a lot of headwinds. I can still imagine it working on its own in some cases. I think there's a lot of people who are paying $75,000 a year for college right now sitting at home on Zoom realizing that they can watch better lectures than they are. And I think a lot of things in higher ed are going to change no matter what the policy is. But I think if A.I. and policy can work in harmony, or let's even say technology and policy can work in harmony, we can get to a much better outcome much more quickly than we otherwise might. And I think — again, to stick on the example of higher ed — I have never seen more energy from the consumers, the people who are going to college or thinking about going to college soon, saying, what am I really doing here and maybe there's some much better way. So that's the kind of pressure where I think things can change super quickly. The prestige of a degree from a top college probably doesn't go away that quickly, but maybe a lot of the rest of it does."
2021-06-11,Ezra Klein,How about housing?
2021-06-11,Sam Altman,"I think it is time to consider radically new ideas there. I mean, it's just so sad to me to watch what San Francisco is doing to itself. I think it's so incredibly unfair. It really bothers me that it's people who would describe themselves as liberal and progressive that are most in the way of, I think, the justice that comes from affordable housing and access to opportunity with that in the places where it matters most. And I think we should continue to fight that as hard as we can. But we should also realize that maybe we're dealing with people here who are not reasonable actors. And there may be other cities where it's just much easier to solve this problem. I think there's been a mini-boom in new real estate startups as people in the pandemic have thought about leaving the big cities, where people can realize, wow, I can build a new house in some other city for not that much money and I can get exactly what I want. And I can now do it quickly with technology, surprisingly affordably, and maybe a competitive market will do its thing here."
2021-06-11,Ezra Klein,"You're giving me, though, the answer here that I give to people when I'm in a technoskeptic kind of mood, which is, well, the problem in our way on a lot of things is not that the technology isn't there. It's that the political will isn't there. It's that the structure isn't there. It's that there are other impediments. So you have this piece about how A.I. is going to move all this power from labor to capital and it can transform humanity way for the better or not. That's where the Moore's law for everything idea comes in. What is the role of the system you're building here? How is it going to help housing or any of this?"
2021-06-11,Sam Altman,"I mean, technology has already done a lot to help housing, I would say. And one specific thing that it did was enable remote work. And so, fine, a city like San Francisco is not going to be reasonable on policy. People can live elsewhere and still work for the most important companies in the industry that have been here. And they are freed from some of the geographic constraint. Look, remote work certainly does not work for everything and it's not something that I'm personally excited to do. But I think it does work for a lot of things and a lot of people. And I think that's an example of where faced with a seemingly intractable political reality, technology produces a solution that is a total curveball and not that imaginable a few decades ago."
2021-06-11,Ezra Klein,"Tell me about the underlying idea that A.I. will move not just money, but also power from labor to capital."
2021-06-11,Sam Altman,"I think the best way to frame this is this idea that the marginal cost of an A.I. doing work is close to zero once you've created this model, which requires huge amounts of capital, and expertise, and difficulty, and data to do. And I think it's a very interesting question about who should benefit from that if — who generates the data or whatever. But once you train this model — maybe you used to have to pay an expert lawyer $1,000 an hour to answer a question or a computer programmer $200 an hour. And there weren't that many and they had a lot of — you needed it and that was the market. That was what it was worth. And that was what people were able to command. But maybe now it costs a couple of cents of electricity for the computer to think or less. And you can do it as many times as you want. You can get the answers that no human could come up with. Labor then — in this case, extremely high-skilled and highly paid labor — all of a sudden has a lot less power, because the services are avA.I.lable at a wildly different cost."
2021-06-11,Ezra Klein,"So you write in the piece and that three things follow from that. One is that phenomenal wealth can get generated, because goods and services can become so cheap and so widely deployed that very rapid social change follows that because you're just undergoing an extraordinary upheaval in the economy. And so then you have this kind of branching possibility, which is either very high standards of living for everyone or not. Some people get better entertainment and whatever, but they never get back the dignity, they never get back the money they were making."
2021-06-11,Sam Altman,Yeah.
2021-06-11,Ezra Klein,How do you assess the political economy of A.I.?
2021-06-11,Sam Altman,I'm curious if you agree with the first two — if you think the first two things are going to happen.
2021-06-11,Ezra Klein,Phenomenal wealth and social upheaval?
2021-06-11,Sam Altman,Yeah.
2021-06-11,Ezra Klein,"If you're right about what the systems can do, definitely that will create wealth, right? I mean, it just has to. I think it's almost mechanical. Upheaval to me is about how quickly it happens. And I suspect it could be technologically possible far before it is actually happening, or that the dispersion of the technology might be a lot slower than the realization of it. So you mentioned a minute ago lawyers. You ask lawyers questions because you need answers. You also ask lawyers questions because if you don't get the answers from them, you might get sued. And so the question of when do you not get sued for asking the question of your system, as opposed to of the lawyers, is to some degree a regulatory and judicial issue more than it is even a technological one."
2021-06-11,Sam Altman,"Yeah, that's why I assume there will be these hybrid models where an A.I. is helping the lawyer, but the human lawyer and the existing system of the protection that you get from that is still the one giving you the answer. So maybe what happens is you still go to a lawyer, but that lawyer can be a factor of 10 or 100 times more efficient and they don't need the staff of researchers they have today."
2021-06-11,Ezra Klein,"So I could see that and I think that makes sense. But that'll reduce the amount of economic upheaval it brings. Health is a really big one. I've done a lot of work on health care over the years. I find it very hard to think through what the effect will be here. But let me put it this way. I've got a long running argument that we think too much about price in health and not enough about value. So budget work in Washington is always dominated by these graphs that show like by 2070, 50% of the economy is on health care. And people will say that's wild. We can't have 50% of our GDP going to health care. But how wild it is depends on what that is buying you, what the value of it is."
2021-06-11,Sam Altman,For sure.
2021-06-11,Ezra Klein,"If we're spending 40% of GDP on health care, but everybody is living a healthy life till 200, maybe it's fine. If we're getting what we're getting now, it's terrible. And so one of the questions I don't really know how to answer is does A.I., whether or not it is being used with doctors and nurses or otherwise, does it actually change radically the development of drugs, the device orientation and devices we're able to use?Do we get way better at spotting things on radiology, which I think is pretty likely. So how do you change the value? You might not see that show up in wealth statistics exactly, but it would be a huge improvement in utility and how humans live. And so that stuff is very interesting."
2021-06-11,Sam Altman,"I think wealth is definitely an imperfect metric. And I think price is even more imperfect. I totally agree with you, by the way, if we had a magic pill that everybody could take and have an extra 20 years of perfect health span and that cost 40% of GDP or whatever, maybe it's fine. So I think these metrics are all imperfect. And what we really want is someone's overall quality of life, and happiness, and fulfillment, and contentment and whatever. But I think for something numeric that we can measure today, wealth is the best thing. And I do think in terms of the branching that you talked about, somehow or other — this is maybe the techno-optimist in me — almost everyone's lives are going to get better. People will demand it. And the question is, what form is that going to go in? I don't actually subscribe to the Silicon Valley UBI will solve all problems. We can just do that and stop talking about it. I think it's actually a small part of the solution. But I think we're going to do it. I think somehow or other that's going to happen. But for me then the question is, how are we going to construct the rest of society in a way that people have the dignity, the ability to decide where we all collectively go that is so important?"
2021-06-11,Ezra Klein,"Well, let's talk about UBI for a minute, because the place you go with this in your piece is toward something you call the American Equity Fund. I think actually a useful way to think about it is universal distribution of wealth."
2021-06-11,Sam Altman,Yeah.
2021-06-11,Ezra Klein,Talk through that idea.
2021-06-11,Sam Altman,"A thing that I did not understand when I was a kid is the difference between salary and equity ownership. And I assumed that the way people got rich was salary. And I knew that people traded stocks, but I didn't think it was a big thing and not that important and it was basically about you get paid monthly. And really I think the way that value gets created is the compounding effects of equity, basically. So that's the pro case on capitalism is that that works really well. The negative case is most people don't own very much equity or land. I said in the piece that I think land and shares of companies are going to be the two dominant sources of wealth in the future. And so better than a government paying you some fixed fee every month is where citizens of a country own a slice of the important asset classes of the future. And that that's how wealth gets redistributed, which I think is different than redistributing the equivalent of salary. Much more important."
2021-06-11,Ezra Klein,"I probably would agree with that. The problem I see — and this is why I asked very specifically about the political economy of A.I. So in this piece, you make an argument about how it'll change the distribution of money and how could you tax around that. But this is one of those places where power really matters. And so another way of saying what you're saying is that A.I. will make owners of capital and land much more powerful. And we are then somehow going to tax the owners of capital and land much more aggressively. And one of my, certainly, concerns about the world being sketched here is that my sense as a political person is you would somehow need to get a more equitable distribution of power in order to have the more equitable distribution of resources."
2021-06-11,Sam Altman,"Totally agree with the statement about the equitable distribution of power being the important thing. Unlike paying cash transfers, I think if you're actually transferring shares in these companies and the accumulating wealth there, or the ownership of land, or whatever else, that actually transfers some of the power over time, too."
2021-06-11,Ezra Klein,"But in order to get to that policy, you need the power to transfer first."
2021-06-11,Sam Altman,"Yeah, here's where we're deeply out of my area of expertise. So I will ask you, what do you think it takes to get something like this in place?"
2021-06-11,Ezra Klein,"I don't know. I'm not sure you can. We were talking about this before we started the show, but something I've been starting to try to think through is how do you have a technologically aware progressivism. Have you ever read the book, ""Fully Automated Luxury Communism""?"
2021-06-11,Sam Altman,I've heard the phrase.
2021-06-11,Ezra Klein,"The argument it makes is that the set of technological changes coming down the pike — of which A.I. is central to that, but it's not the only one — they actually make a case for much more radical form of public ownership. He's using communism a little bit facetiously. But forms of communism, of socialism make a lot — of planning, of ownership, make a lot more sense under the technological structure that's coming. But the question then becomes in order to have that happen, I think you actually need to set up the politics before the technology hits. Because if the technology hits first and then it just pools power among the people who own it, well, power increases its own wealth and increases its own power. And then we got these crazy A.I.s that are out there trying to manipulate public opinion and whatever else. It can get a little bit dystopic. And it's not all going to happen overnight, but I do think this is a real — it's a real question for me and it's one reason I've become — in some ways what I want to see is a politics on the left, or on the progressive side, that is simultaneously more pro technology but is trying to take a stronger hand in predicting and then guiding that technology. But that's really hard. I mean, we can't pass even simple things right now. So the question of how we do it — I mean, I don't know how we do basic climate change legislation. So how we completely restructure the way taxation works in this country when one of the two major political parties has literally signed a pledge to never raise taxes for any reason on anyone at any time? I don't really think it's a question that is answerable. But I am worried about what happens if some of these things hit and we haven't come anywhere near an answer."
2021-06-11,Sam Altman,"Our version of this is that technology is good, capitalism is fine, but you can put structures in place now that get you to the goal that you want. And I think we have this thing where hopefully we design a structure that let us benefit from capitalism to bring the thing into existence and I think that we couldn't do without it. But once we've hit certain return thresholds, we have already figured out how our public ownership is going to work. The ink is dry on that. And I think that's cool. And I think more people should be doing things like that. And I think there may be versions of that that we could get done as a policy conversation, where you say this is how it's going to work now. Don't freak out too much. But here's where it's going to transition to over time."
2021-06-11,Ezra Klein,"One of the problems, though, with the policy conversation is that this is the set of issues it's way behind on. So there are topics we know how to talk about in Washington from a policy perspective. Taxes is one of them. But I mean, you were saying earlier in the show that at least the power of these systems is on an exponential curve."
2021-06-11,Sam Altman,"Yeah, taxing income — fighting over the rate on income tax is so the wrong way to be thinking about this."
2021-06-11,Ezra Klein,"And my concern is that to the extent there is a Washington conversation over — artificial intelligence is one of the set of technologies I'm interested in and worried about. But to the extent there's a Washington conversation about it — and you probably know this better than I do — my sense is it has three prongs. It's about algorithmic bias, which is important."
2021-06-11,Sam Altman,Yep.
2021-06-11,Ezra Klein,It's about competition with China.
2021-06-11,Sam Altman,Yep.
2021-06-11,Ezra Klein,And it is on the margins about how do you subsidize maybe academic access to enough computing power.
2021-06-11,Sam Altman,The first two seem to dominate.
2021-06-11,Ezra Klein,"But yeah — so I mean, it's basically it's bias, it's national defense, and then it's some other stuff, like a grab bag. It's not really about what do you do if society begins changing really fast. And also — and I mean this as exactly as important — what do we want from it as a public and how do we get that from it?"
2021-06-11,Sam Altman,"Let me tell you something. If you walk into one of these rooms and you — you know, they're like, oh, I want to learn from you. What should we be thinking about A.I.? And you say anything other than it's really important that we remain competitive with China and it's really important that we have fair, transparent, unbiased systems. If you say those two, you're like, oh, it's great, it's what we want. And those are really important for sure and we think a lot about those. But if you say it's also important that we talk about 20, 30, 40, 50 years from now when we have these systems that are way more capable than any human leaving from Earth to go off and explore, and colonize the universe, and figuring out what the role for humans are and how we want to set up that society, you get a real eyes glazed over look and sort of slowly back away from."
2021-06-11,Ezra Klein,Do you believe in 30 years we're going to have self-intelligent systems going off and colonizing the universe?
2021-06-11,Sam Altman,"Look, timelines are really hard. I believe that will happen someday. I think it doesn't really matter if it's 10 or 30 or 100 years. The fact that this is going to happen, that we're going to help engineer, or merge with or something, our own descendants that are going to be capable of things that we literally cannot imagine. That somehow seems way more important than the tax rate or most other things."
2021-06-11,Ezra Klein,"But in a weird way I'm not sure it is. I mean, it is more important than the marginal tax rate on high income. I very much agree with that. But I do think sometimes there is a tendency to say that the intermediate questions between here and AGI don't matter that much. And I think they do, because I think that they matter very much in terms of what the future systems of the world will do. I'll give an example of this. I think there's a lot of question on how do we distribute income, which if you could get the politics right to do it, is a quite manageable question. If you gave me the wand, I could work with five tax experts and figure it out. The question of how do you distribute dignity and status in that society —"
2021-06-11,Sam Altman,"100%,"
2021-06-11,Ezra Klein,"—is unbelievably harder. So for instance, I'm pretty high on the idea that maybe wage labor is an intermediate step in human society and wouldn't it be great if we got past the point where it was widely needed. But that is much more a cultural question in the worlds we're talking about here than it even is an economic one."
2021-06-11,Sam Altman,"100%. I think this is such a harder problem and in many ways more important problem than how we're going to redistribute income and wealth. And this is why I think the intermediate things do matter a lot. It's because how we — the steps that we take now will determine who has power and voice and input in the long-term decisions that are some of these rare, irreversible human decisions that we'll ever make. You are probably sitting there, or at least part of you is sitting there, looking at me, thinking like, I probably don't really believe this guy all the way. But if he's right, fuck that dude, why does he get to decide what happens with A.I.? That's not — that doesn't seem fair. And you're totally right. And if I were you, that's totally what I would think. And more importantly than that, everybody has a right to a say in the future, and the dignity of being part of it, and getting to live their lives the way that they want. And I don't think we've ever quite faced a technology like this and I don't think we quite know what it looks like to have equitable input in those decisions."
2021-06-11,Ezra Klein,"And I'll say more than that. I don't even know if what you're telling me is true, right? What I think is that there is a probability it is true that I cannot quite assess."
2021-06-11,Sam Altman,Right.
2021-06-11,Ezra Klein,"And if that probability is significant, and I think it at least is for the economic side of this — I don't know about the space colonization side or on what time frame — then it's important. Two things. One is that I think an endless truth of human society, at least in the age of capitalism, is that status follows money."
2021-06-11,Sam Altman,Yep.
2021-06-11,Ezra Klein,"And so getting the money right is going to be crucial to getting the status right, which is why the redistribution questions are really, really important. But the second then becomes, what are the governance structures that might work here, or the participatory or representative structures that might work here? Because I've been pushing so far in this conversation on the problems of capitalism. It isn't like I'm confident that a state-run A.I. is a great idea too. I don't really want China running one of these systems and I wouldn't have wanted Donald Trump running —"
2021-06-11,Sam Altman,No.
2021-06-11,Ezra Klein,—one of these systems. And so the governance question here strikes me as really hard and not one where I'm comfortable with either of the dominant options.
2021-06-11,Sam Altman,"I think we need something fundamentally new. We're trying to run some experiments and do some investigatory work here. I don't think anyone's ever figured out what a global democratic governance system looks like. But I think we better try to figure that out soon. And there's a lot of reason to be pessimistic. Look, I also — I think that — I think everything that I've said is going to happen, but we could hit some wall. It's always possible and maybe none of this happens. But if we shift the frame a little bit and talk about — we were talking about the environment earlier, so let's go back to that. We could have had nuclear energy deployed everywhere, the full generating capacity needed for Earth and then some, quite a long time ago. And we don't. And I think that's a real shame of policy, and, at least in this country, some quite bad political decisions on the left and an understandable but misplaced fear of technology. And we're paying an awful price for it right now. And if we take all the speculation out of something like A.I. and just say can we look at an example of a collision of technology, and policy, and politics, and public perception, and economics, and a whole bunch of other things, too, and see where we got the governance wrong, we don't need to make up examples for this. And so that is a scary thing."
2021-06-11,Ezra Klein,"Yeah, and that, I think, is also an important thing. I mean, we were talking earlier about, well, what if A.I. can solve in the reasonably near term — 10, 20 years — pretty big societal problems? A lot of the problems — and this came up in our conversation about health and housing. No computer system can solve a political problem, but it can create alternatives. And then we do have a lot of genuine technical and scientific problems that do need to be solved. And if we don't get governance right, if we don't get public input right, if people don't feel they're going to benefit from this, then we're also going to delay deployment of technologies. There's a lot of this where you're going to need regulatory decisions for the things to be deployed. And so that becomes a question of to what degree are people comfortable with it. And not just Phoenix is going to allow self-driving taxis for a little bit to show they can. But do you actually begin to say, we are going to create the systems, the liability systems, et cetera, that can allow this to go all through society or not? And I don't know the answer to any of that. I'm partially doing these shows for this reason that it's just not something that I think is enough part of the political conversation. But then I look at the past year and I see a lot of failures of governance followed by a gigantic success of mRNA vaccines. And if we had not gotten that technology right, we would just be in a continued mess here. And so getting the technologies right fast enough that they can actually solve the problems, you can really see there what it would have meant if we had not — I don't think regulators did a crazy good job on this, but we got the vaccines out."
2021-06-11,Sam Altman,"I don't think they did a crazy good or a crazy bad job on this one, but I do think they've done a crazy bad job on a lot of other medical innovations that could have saved a lot of lives where something has gone wrong in terms of our risk tolerance for those. Now, look, it does warm my heart that when a heroic effort is needed, we can sometimes step up. But I do think it is worth thinking about how many things like mRNA vaccines we have missed, or are still delayed, or came 10 years too late."
2021-06-11,Ezra Klein,"But isn't there, in some of the things you're talking about here, a cultural question that afflicts your own industry pretty centrally? I wouldn't say that the people who run major technology companies are heavily anti-tax. Not in general. But there is a pretty heavy strain of the venture capitalists and others who love their carried interest tax, who get — and it's not just the taxes — who get real angry at the idea of taxes as a moral phenomenon. That I think is an important dimension of this. Get real angry at the idea that being taxed might imply they don't deserve what they have. There was a vote a couple of years ago where folks were comparing this to Naziism. That if this stuff is going to deploy at the rate people wanted to out here, I think there's going to need to be a different politics in Silicon Valley."
2021-06-11,Sam Altman,"I actually think that most people in the industry are pretty happy to pay taxes. Sure they may advocate for less. There's two comments that you hear a lot, one I agree with and one that I don't. One is that I'm happy to pay taxes, I just wish they were better spent. And because they're so badly spent, I don't want to pay any. While California is in shambles, why am I paying 13.3% tax? This is embarrassing. We have terrible schools, terrible roads, traffic. The money is being so wasted. I don't want to pay anymore. And so I'm moving or I'm going to try to figure out some other tax setup or whatever. I don't agree with that. You hear it a lot. I do understand where people are coming from. They're like, well, I can allocate the capitol better than the government can to still help people. The one that I do agree with is I think there's a lot of people in the tech industry — founders, venture capitalists, whatever — that really object to messaging like billionaires should not exist. Which is very different from saying billionaires shouldn't be taxed. I would love to have trillionaires in the world if they were paying a lot of tax. And I think everybody else should want that, too. Now, there is a challenge that comes with the point you were making about a lot of power comes with that, too."
2021-06-11,Ezra Klein,"Yeah, that's what I was going to ask."
2021-06-11,Sam Altman,"But on the whole, as long as people are paying taxes and creating economic value, I want more of that. And I think most members of society should want more of that, too, if we can figure out how to make sure that we have fair access to power and voice and governance. And maybe what you would say is that's not possible with that level of contribution of wealth."
2021-06-11,Ezra Klein,"Yeah, I would say I think there might be a more deeper tension there than you think. So take the couple of richest people in the world right now. Is it such an accident that Jeff Bezos and Bill Gates live in Seattle, a state with no income tax? It's all fine to be for taxes in some conceptual way. But in practice, very rich, very powerful people living in a state — and it's not like they're helping to get an income tax passed. Or that Elon Musk just re-domiciled himself in Texas to pay, at least in part, lower taxes. I mean, power speaks. And I mean, my worry — I don't think there should be a trillionaire. Not because I don't want somebody to invent the thing that would have made them a trillion dollars — I do. But one, the marginal utility of having a trillion dollars is crazy low for one person. That should just be distributed to people who then won't die from malaria because they have some money. But two, that's a lot of power for one person to have. And I think if I have one critique of your approach to politics on this stuff it's that I think you take policy seriously and power less seriously. But one of my observations from covering policy for a long time is policy reflects power. It doesn't precede it."
2021-06-11,Sam Altman,"I can totally believe that. again, I think if there is a big bug in my thinking, this is likely it. I'm very sympathetic to the idea of being wrong here. I doubt that it's an accident that those people all live in no income tax states. But I also think — I haven't talked to Elon about this, but I know him well enough that I would bet that his bigger reason was just an increasing unfriendliness in California towards his businesses and a real disagreement with how they were handling his company's ability to be successful. And that is, I think, a real shame for all of us. No one, as California residents, none of us benefit from having fantastic companies move out of the state. That is a real shame. And that was power that the California legislature had that I think they badly misused that preceded the politics. And I wish that didn't happen. On the trillionaire question — we were talking earlier about health care. Let's say someone invents — some scientist invents something that gives everybody on Earth 20 years of extra great health span. And they want to make it phenomenally expensive. But they create $100 trillion of value and become a one trillionaire in the process. I would cheer them on."
2021-06-11,Ezra Klein,I just don't think that's the binary question. It's hard for me to imagine —
2021-06-11,Sam Altman,But then how do you decide how much they're allowed to make?
2021-06-11,Ezra Klein,"I mean, through politics, right? It's hard for me to imagine the person who would invent that, who would be dissuaded from inventing that because they would only end up with $50 billion as opposed to a trillion dollars."
2021-06-11,Sam Altman,So we're going to say $50 billion is OK?
2021-06-11,Ezra Klein,"I don't think we're going to find an optimal. But again, I'd want to know a little bit more about both the product and the power distribution before I became comfortable with their being trillionaires because they can cause a lot of damage. And let me argue that there's actually more of a friction here even than we've let on. So your point about Elon Musk is well taken. I'm not saying anything is mono-causal. But I think something going on in California in the relationship between the state and the technology companies that are centered here, and something that's not going to be good for either side, is that as the increase in wealth inequality here rises, the sentiment changes. The politics change. And so part of the reason there's more friction is because people are more pissed off. A different example and one that I think is in some ways easier to see the problems with is Mark Zuckerberg gives a huge amount of money to a hospital, names it after himself, and then there's this fight and the — I think it was the board of supervisors that voted to condemn the name of the hospital. And part of what's just going on there is people are angry about the power he has. I don't think it was a bad thing he gave that money to the hospital. And given the way we've named things after people forever — I mean, go to a sports stadium around this country — I didn't even really care that it got named after him. But there is a tension that begins to emerge and poison relations as people begin to feel that the wealth of the economy is unequally shared. And I think that's a pretty big issue. I mean, one place my politics has probably been moving in recent years is I would like to see a much more radically pro-technology progressivism. But I think in order to do that, you might need a much more economically aggressive form of progressivism. That's why I'm interested by the fully automated luxury communism concept, because I wonder if in order to create enough societal comfort to have the technological society I think is possible, you actually need a much more radical form of equality than we have. I did this show with Ted Chiang, the science fiction writer. He made this point where he said, I think most of our fears about technology are fears about capitalism. And I depart from that in the sense of if you live in China, or a lot of other countries, I don't think that's true, and even sometimes not in this country. There's things to worry about with the state, too. But it is true for a lot of things here, and I think it actually holds back technology and the degree to which people are willing to adopt it in a way that is bad for everyone."
2021-06-11,Sam Altman,"So on this point, I think we're in perfect agreement. I think we have to do this. I think technology can make the world unimaginably great, but it needs a policy and power tweak to have that be distributed in a way where it can happen at all and in a way where it can happen justly. So I agree with that. I also agree that the heads of the biggest technology companies probably do have too much power, or do have too — I'll take out the probably. In the Industrial Revolution, when the joint stock corporation was created as this second order sovereign entity, everyone was OK with that, because it was second order and the real sovereign had more power. But I think you can certainly make a case now that the giant tech companies are more powerful than many countries, certainly not the U.S. yet. But they don't have any kind of democratically governed system. So yeah, I mean, that causes me deep discomfort."
2021-06-11,Ezra Klein,"When I asked Ted Chiang about AGI, he said something I've been thinking about since. Which is that could we invent it? Maybe. Will we invent it? Maybe. Should we invent it? No. And the reason he said no was that long before we have a sentient generally intelligent A.I., we'll have A.I. that can suffer. And if you think about how we treat animals, or even just think about how we treat computers, or, frankly, workers in many cases, the idea that we can make infinite copies of something that can suffer that we will see in a purely instrumental way is horrifying. And that fully aside from how human beings will be treated in this world, the actual A.I. will be treated really badly. Do you — I mean, you're somebody who thinks out on the frontier of this. I know this part of the conversation is going to turn some listeners off, but I think it's interesting. Do you worry about the suffering of what we might create?"
2021-06-11,Sam Altman,"Yes, a lot. And we'd call that model welfare, but it's the same idea. Fundamentally, I think people worry are we going to mistreat the A.I. like we mistreat animals or is the A.I. going to mistreat us like with animals? That's —"
2021-06-11,Ezra Klein,Yes.
2021-06-11,Sam Altman,Mistreat animals. That's the — you can put people in one of those two buckets usually.
2021-06-11,Ezra Klein,It really says a lot about how we treat animals. s
2021-06-11,Sam Altman,"I think we're going to have a lot more vegetarians soon, I hope. There's this great A.I. short story called, ""Crystal Nights."" And it's one of the ways that A.I. could be developed is what people call multi-agentally. Basically, rerun evolution and simulation. In an environment like that, you probably do get enormous amounts of suffering. A lot of other approaches I think you do, too. Maybe if a reinforcement learning agent is getting negative rewards, it's feeling pain to some very limited degree. And if you're running millions or billions of copies of that, creating quite a lot, that's a real moral hazard. The technical path that I think — currently think is most likely to get us to one of these AGI-like systems is one where you don't have tons of copies of agents. And so maybe there's a moral issue with causing any pain. But I sleep a little bit better at night knowing that we're not torturing trillions of agents in simulation. I also think — and this is deep into the realm of speculation — that certainly, I think, as humans have become more knowledgeable, and wise, and smarter, we seem to be less willing to inflict random pain on each other and maybe animals than we were —"
2021-06-11,Ezra Klein,"See, I think animals is the counter-argument to that. Because of our capabilities, the number of animals we mistreat is so wildly out of proportion of what we've ever been able to do before, because we can keep them alive in sheds with antibiotics and stuff, that our thinking about it is probably better, but what we're willing to outsource is worse."
2021-06-11,Sam Altman,"I don't know. I have noticed over the last few decades more and more of my friends consistently become vegetarian. I think that's a positive. You used to not have a lot of vegetarians in the Western world. We have the privilege and luxury to be able to not do that. We now increasingly have the technology to have fake meat. But I think we also have people who are just more conscious about these issues. And I hope that we are in the late stage of the twilight of factory farming. I mean, look, we're heading into the deepest philosophical questions that have been on humanity's mind for a very long time, but maybe have never been as relevant or as decision-relevant as they are now. But I think a lot of these things really come down to, A, do you believe that a sense of self exists at all or is everything just like — there's this body, and there's this brain, and there's energy flowing through a neural network in your head like there could be in a computer. And as that is running, it creates this illusion of a sense of self that is getting tortured but it really is not there at all and that it's all the same thing. That's a philosophy that I happen to mostly feel is the most true. There's others who would say quite the opposite, which is that there will never be a sense of self in a — there is a self. There's a soul, whatever. And that will never exist in a computer. And so all of these questions are irrelevant. There's no way that any neural network running in Silicon could ever suffer and it's a ridiculous question. People have quite different opinions on this."
2021-06-11,Ezra Klein,"Why try to make it generally intelligent at all? Geoff Hinton, one of the fathers of neural networks, he had this quote in the book ""Genius Makers,"" that I recently read, where he just says, why do you want the robot digging your ditches to know about baseball? Why create the potential for this kind of intelligence and kind of suffering at all? Why not just create narrow worker machine learning programs?"
2021-06-11,Sam Altman,"I have a lot of answers to this question, so I will start with a few. Number one, one thing that I really want is new knowledge creation. It is one thing to say you can have an A.I. that is as good as any human A.I. doctor. It is another to say you can have an A.I. that can solve all human disease in a way that humans are just not capable of doing. And I think you need a generally intelligent system to do that, to generate new knowledge, to learn new things, to do things humans can't do. And it will need to pull together expertise across many different areas that no human is capable of holding in their brain at once — or even a team of humans — to do. So what a depressing thought to say that we're going to limit ourselves to what humans are capable of rather than benefit from everything that we can build better tools for. We build tools so that we can do better than we can do with our hands digging up the dirt or whatever. All of this, everything we have here is because we started digging up the ground, finding stuff. And we made this room. We made this microphone. We made the internet. We have done incredible work with tools that have let us shoot past what we'd be capable of without them. And let's not stop. That would really be depressing to me. Two is that someone's going to do it. The upside of these systems are such that Geoff Hinton can certainly decide not to try to build generally intelligent systems, but someone's going to do it. So I think there is no future that doesn't have these systems in it. And so we have to talk about how we want to use them, what their rights are, what we want the world to look like, the universe to look like with them."
2021-06-11,Ezra Klein,"If we're sitting — if we get together for coffee in 20 years and none of it happened, it all just stalled out, what is the likeliest reason you're going to give me for it?"
2021-06-11,Sam Altman,"If it doesn't happen in 20 years, I'll talk about a bunch of little technical things that could go wrong. If it doesn't happen in 100 years, then I was wrong and there is some magic to being a human like we are, living in a simulation, or created by some God or something like that. There's something going on that is not just physics."
2021-06-11,Ezra Klein,"It's that big? You think that it will — physics is going to be up for debate at that point? Because some people would say, how far it can get with neural networks pulling in data to do predictive learning. There's just going to be a limit on that."
2021-06-11,Sam Altman,"Well, here you are, though. I mean, it works for you."
2021-06-11,Ezra Klein,"Yeah, but only so well. You should see me — you should see me on my off days."
2021-06-11,Sam Altman,"Sure. But if we can have an Ezra, that's pretty great. And my understanding, my belief is that you are energy flowing through a neural network. That's it. Perception comes in. It cycles around a neural network in your head and you — some muscle of yours moves. But that's it. That's the whole Ezra. And that is replicatable energy flowing through a neural network. And is replicable in a very big computer. It'll take us a while to figure out there's a lot of complexity there maybe. But yeah, if it doesn't happen in a few hundred years, or 100 years even, then some axiom I believe is wrong."
2021-06-11,Ezra Klein,I'm going to go meditate on the idea that I'm energy flowing through a neural network.
2021-06-11,Sam Altman,"It is a little depressing. But if you can just let this one idea go that there's a special self, there's an Ezra that controls all of this. And just say, there's nothing here. It's just the system and it has this side effect of me thinking I'm an Ezra, then it all kind of works."
2021-06-11,Ezra Klein,I think that's a good place to end. So what are three books you'd recommend to the audience to help the energy flow slightly differently through their neural networks?
2021-06-11,Sam Altman,"Can I reco — both because I think they're more likely to get read and I think they're more relevant to this conversation. I don't think there's any great books about A.I., but there are good short stories. So could I recommend —"
2021-06-11,Ezra Klein,Go for it.
2021-06-11,Sam Altman,"—short stories? ""Crystal Nights"" by Greg Egan, ""The Last Question"" by Isaac Asimov, and ""The Gentle Seduction"" by Marc Stiegler. They're all about the development of a super powerful A.I. in very different ways. Actually, if I can recommend a bonus fourth one. This is a blog post, not a short story, but it really touches on a lot of this societal governance power issues we're talking about relative to A.I. ""Meditations on Moloch."" It's a blog post on Slate Star Codex. I strongly recommend that one."
2021-06-11,Ezra Klein,"Sam Altman, thank you very much."
2021-06-11,Sam Altman,Thank you.
